<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 4: Storage & Data Processing - Staff Engineer Prep</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/animations.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        /* Interactive Kafka Demo */
        .kafka-container {
            background: linear-gradient(135deg, #1a1a2e 0%, #0f3460 100%);
            border-radius: 1rem;
            padding: 2rem;
            margin: 1.5rem 0;
        }

        .kafka-controls {
            display: flex;
            gap: 1rem;
            justify-content: center;
            flex-wrap: wrap;
            margin-bottom: 1.5rem;
        }

        .kafka-btn {
            padding: 0.75rem 1.5rem;
            border: none;
            border-radius: 0.5rem;
            cursor: pointer;
            font-weight: 600;
            color: white;
            transition: all 0.3s ease;
        }

        .kafka-btn.produce {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }

        .kafka-btn.consume {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }

        .kafka-btn.batch {
            background: linear-gradient(135deg, #f6ad55 0%, #ed8936 100%);
        }

        .kafka-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }

        .kafka-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none;
        }

        .kafka-viz {
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 2rem;
            margin: 1.5rem 0;
            min-height: 300px;
            flex-wrap: wrap;
        }

        .kafka-producers, .kafka-consumers {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            align-items: center;
        }

        .kafka-producer, .kafka-consumer {
            width: 80px;
            height: 80px;
            border-radius: 1rem;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            font-size: 0.8rem;
            transition: all 0.3s ease;
        }

        .kafka-producer {
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }

        .kafka-consumer {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }

        .kafka-producer.sending, .kafka-consumer.consuming {
            transform: scale(1.1);
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.4);
        }

        .kafka-producer .icon, .kafka-consumer .icon {
            font-size: 1.5rem;
            margin-bottom: 0.25rem;
        }

        .kafka-topic {
            flex: 1;
            min-width: 300px;
            max-width: 500px;
        }

        .kafka-topic-header {
            text-align: center;
            color: white;
            font-weight: 600;
            margin-bottom: 1rem;
            font-size: 1.1rem;
        }

        .kafka-partitions {
            display: flex;
            flex-direction: column;
            gap: 1rem;
        }

        .kafka-partition {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 0.5rem;
            padding: 0.75rem;
            position: relative;
        }

        .partition-label {
            color: #a0aec0;
            font-size: 0.75rem;
            margin-bottom: 0.5rem;
            display: flex;
            justify-content: space-between;
        }

        .partition-messages {
            display: flex;
            gap: 4px;
            height: 40px;
            align-items: center;
            overflow-x: auto;
            padding: 0.25rem;
        }

        .kafka-message {
            min-width: 35px;
            height: 35px;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7rem;
            font-weight: 600;
            color: white;
            transition: all 0.3s ease;
        }

        .kafka-message.new {
            background: linear-gradient(135deg, #ffd700 0%, #ff8c00 100%);
            animation: message-arrive 0.3s ease;
        }

        .kafka-message.committed {
            background: linear-gradient(135deg, #48bb78 0%, #38a169 100%);
        }

        .kafka-message.uncommitted {
            background: linear-gradient(135deg, #4a5568 0%, #2d3748 100%);
        }

        .kafka-message.consuming {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            animation: pulse-consume 0.5s ease;
        }

        @keyframes message-arrive {
            from { transform: scale(0); opacity: 0; }
            to { transform: scale(1); opacity: 1; }
        }

        @keyframes pulse-consume {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.2); box-shadow: 0 0 15px rgba(102, 126, 234, 0.6); }
        }

        .consumer-offset {
            position: absolute;
            bottom: -25px;
            height: 20px;
            width: 2px;
            background: #667eea;
            transition: left 0.3s ease;
        }

        .consumer-offset::after {
            content: 'â–²';
            position: absolute;
            bottom: -5px;
            left: -6px;
            color: #667eea;
            font-size: 0.8rem;
        }

        .kafka-stats {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            justify-content: center;
            margin-top: 1.5rem;
        }

        .kafka-stat {
            background: rgba(255, 255, 255, 0.1);
            padding: 1rem 1.5rem;
            border-radius: 0.75rem;
            text-align: center;
            color: white;
            min-width: 100px;
        }

        .kafka-stat-value {
            font-size: 1.25rem;
            font-weight: 700;
            color: #ffd700;
        }

        .kafka-stat-label {
            font-size: 0.75rem;
            color: #a0aec0;
            margin-top: 0.25rem;
        }

        .kafka-log {
            background: rgba(0, 0, 0, 0.3);
            border-radius: 0.5rem;
            padding: 1rem;
            max-height: 120px;
            overflow-y: auto;
            font-family: 'Fira Code', monospace;
            font-size: 0.75rem;
            color: #a0aec0;
            margin-top: 1rem;
        }

        .kafka-log-entry {
            padding: 0.2rem 0;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        .kafka-log-entry.produce {
            color: #48bb78;
        }

        .kafka-log-entry.consume {
            color: #667eea;
        }

        .kafka-log-entry.commit {
            color: #f6ad55;
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="logo">StaffEngPrep</a>
            <ul class="nav-links">
                <li><a href="../coding-rounds/index.html">Coding</a></li>
                <li><a href="index.html" style="color: var(--primary-color);">System Design</a></li>
                <li><a href="../company-specific/index.html">Companies</a></li>
                <li><a href="../behavioral/index.html">Behavioral</a></li>
                <li><a href="../generative-ai/index.html">Gen AI</a></li>
            </ul>
        </div>
    </nav>

    <div class="layout-with-sidebar">
        <aside class="sidebar" id="sidebar">
            <nav class="sidebar-nav">
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Getting Started</div>
                    <a href="index.html" class="sidebar-link">Introduction</a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Core Concepts</div>
                    <a href="module-01.html" class="sidebar-link" data-module="1">
                        <span class="sidebar-link-number">1</span>Scalability Fundamentals
                    </a>
                    <a href="module-02.html" class="sidebar-link" data-module="2">
                        <span class="sidebar-link-number">2</span>Database Systems
                    </a>
                    <a href="module-03.html" class="sidebar-link" data-module="3">
                        <span class="sidebar-link-number">3</span>Distributed Systems
                    </a>
                    <a href="module-04.html" class="sidebar-link active" data-module="4">
                        <span class="sidebar-link-number">4</span>Storage & Data Processing
                    </a>
                    <a href="module-05.html" class="sidebar-link" data-module="5">
                        <span class="sidebar-link-number">5</span>Seminal Papers
                    </a>
                    <a href="module-09.html" class="sidebar-link" data-module="9">
                        <span class="sidebar-link-number">6</span>API Design
                    </a>
                    <a href="module-10.html" class="sidebar-link" data-module="10">
                        <span class="sidebar-link-number">7</span>Numbers to Know
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Patterns</div>
                    <a href="module-11.html" class="sidebar-link" data-module="11">
                        <span class="sidebar-link-number">8</span>Real-time Updates
                    </a>
                    <a href="module-12.html" class="sidebar-link" data-module="12">
                        <span class="sidebar-link-number">9</span>Rate Limiting
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Problem Breakdowns</div>
                    <a href="module-06.html" class="sidebar-link" data-module="6">
                        <span class="sidebar-link-number">10</span>Common Problems
                    </a>
                    <a href="problems/url-shortener.html" class="sidebar-link">URL Shortener</a>
                    <a href="problems/news-feed.html" class="sidebar-link">News Feed</a>
                    <a href="problems/chat-system.html" class="sidebar-link">Chat System</a>
                    <a href="problems/video-streaming.html" class="sidebar-link">Video Streaming</a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Interview Prep</div>
                    <a href="module-07.html" class="sidebar-link" data-module="7">
                        <span class="sidebar-link-number">11</span>Framework Mastery
                    </a>
                    <a href="module-08.html" class="sidebar-link" data-module="8">
                        <span class="sidebar-link-number">12</span>Mock Interviews
                    </a>
                </div>
            </nav>
        </aside>

        <button class="sidebar-toggle" id="sidebarToggle">&#9776;</button>
        <div class="sidebar-overlay" id="sidebarOverlay"></div>

        <main class="main-content">
            <h1>Module 4: Storage and Data Processing</h1>

        <div class="card mt-3">
            <h3>Learning Objectives</h3>
            <ul>
                <li>Understand distributed file systems (GFS, HDFS)</li>
                <li>Learn message queue patterns and use cases</li>
                <li>Master stream vs batch processing trade-offs</li>
                <li>Apply async processing patterns in system design</li>
            </ul>
        </div>

        <h2 class="mt-4">Message Queues</h2>

        <div class="collapsible open">
            <div class="collapsible-header">
                <span>Async Communication Patterns</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <div class="diagram-container">
                    <div class="mermaid">
flowchart LR
    P[Producers] --> Q[Message Queue]
    Q --> C[Consumers]

    subgraph "Benefits"
        A[Decoupling]
        B[Async Processing]
        C2[Load Leveling]
        D[Durability]
    end
                    </div>
                </div>

                <h4>Use Cases</h4>
                <ul>
                    <li><strong>Async task processing:</strong> Email sending, image processing</li>
                    <li><strong>Event-driven architecture:</strong> Microservices communication</li>
                    <li><strong>Rate limiting / smoothing:</strong> Handle traffic spikes</li>
                    <li><strong>Cross-service communication:</strong> Loose coupling</li>
                </ul>

                <h4>Popular Systems</h4>
                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="background: var(--border-color);">
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">System</th>
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Best For</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Kafka</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">High throughput, log-based, replay</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">RabbitMQ</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Feature-rich, traditional queue</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">SQS</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Managed, simple, AWS native</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Redis Pub/Sub</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Fast, in-memory, ephemeral</td></tr>
                    </tbody>
                </table>
            </div>
        </div>

        <h2 class="mt-4">Kafka Deep Dive</h2>

        <!-- Interactive Kafka Demo -->
        <div class="collapsible open">
            <div class="collapsible-header">
                <span>ðŸŽ® Interactive Kafka Message Flow Demo</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <p class="text-muted">Watch how Kafka handles messages across partitions. Produce messages and see consumer offsets track progress.</p>

                <div class="kafka-container" id="kafka-demo">
                    <div class="kafka-controls">
                        <button class="kafka-btn produce" onclick="produceMessage()">ðŸ“¤ Produce Message</button>
                        <button class="kafka-btn consume" onclick="consumeMessage()">ðŸ“¥ Consume Message</button>
                        <button class="kafka-btn batch" onclick="produceBatch()">ðŸ“¦ Produce Batch (5)</button>
                    </div>

                    <div class="kafka-viz">
                        <div class="kafka-producers">
                            <div class="kafka-producer" id="producer-1">
                                <span class="icon">ðŸ“¤</span>
                                <span>Producer</span>
                            </div>
                        </div>

                        <div class="kafka-topic">
                            <div class="kafka-topic-header">ðŸ“‹ Topic: orders</div>
                            <div class="kafka-partitions">
                                <div class="kafka-partition" id="partition-0">
                                    <div class="partition-label">
                                        <span>Partition 0 (key % 3 = 0)</span>
                                        <span id="p0-offset">Offset: 0</span>
                                    </div>
                                    <div class="partition-messages" id="messages-0"></div>
                                </div>
                                <div class="kafka-partition" id="partition-1">
                                    <div class="partition-label">
                                        <span>Partition 1 (key % 3 = 1)</span>
                                        <span id="p1-offset">Offset: 0</span>
                                    </div>
                                    <div class="partition-messages" id="messages-1"></div>
                                </div>
                                <div class="kafka-partition" id="partition-2">
                                    <div class="partition-label">
                                        <span>Partition 2 (key % 3 = 2)</span>
                                        <span id="p2-offset">Offset: 0</span>
                                    </div>
                                    <div class="partition-messages" id="messages-2"></div>
                                </div>
                            </div>
                        </div>

                        <div class="kafka-consumers">
                            <div class="kafka-consumer" id="consumer-1">
                                <span class="icon">ðŸ“¥</span>
                                <span>Consumer</span>
                            </div>
                        </div>
                    </div>

                    <div class="kafka-stats">
                        <div class="kafka-stat">
                            <div class="kafka-stat-value" id="total-produced">0</div>
                            <div class="kafka-stat-label">Produced</div>
                        </div>
                        <div class="kafka-stat">
                            <div class="kafka-stat-value" id="total-consumed">0</div>
                            <div class="kafka-stat-label">Consumed</div>
                        </div>
                        <div class="kafka-stat">
                            <div class="kafka-stat-value" id="consumer-lag">0</div>
                            <div class="kafka-stat-label">Consumer Lag</div>
                        </div>
                        <div class="kafka-stat">
                            <div class="kafka-stat-value" id="partition-balance">0/0/0</div>
                            <div class="kafka-stat-label">Partition Balance</div>
                        </div>
                    </div>

                    <div class="kafka-log" id="kafka-log">
                        <div class="kafka-log-entry">Kafka demo ready. Produce messages to see them flow through partitions.</div>
                    </div>
                </div>

                <div class="card mt-2" style="background: linear-gradient(135deg, rgba(102, 126, 234, 0.2), rgba(118, 75, 162, 0.2)); border-left: 4px solid #667eea;">
                    <strong>ðŸ’¡ Key Concepts Shown:</strong>
                    <ul style="margin-top: 0.5rem;">
                        <li><strong>Partitioning:</strong> Messages with same key go to same partition (ordering guarantee)</li>
                        <li><strong>Consumer Offset:</strong> Tracks what's been processed - enables replay</li>
                        <li><strong>Consumer Lag:</strong> Difference between latest message and consumer position</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>Log-Based Architecture Explained</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <h4>Key Design Decisions</h4>
                <ul>
                    <li><strong>Why Log-Based?</strong> Sequential writes (fast), consumers track own offset, replay capability</li>
                    <li><strong>Partition Design:</strong> Ordering within partition, parallel consumption across partitions</li>
                    <li><strong>Consumer Groups:</strong> Each partition consumed by one consumer in group</li>
                </ul>

                <h4>Guarantees</h4>
                <ul>
                    <li><strong>At-least-once:</strong> Default, may have duplicates</li>
                    <li><strong>At-most-once:</strong> May lose messages</li>
                    <li><strong>Exactly-once:</strong> With idempotent producers + transactions</li>
                </ul>

                <h4 class="mt-3">Kafka Producer - Production Configuration</h4>
                <div class="code-block">
                    <code>
# Python - Kafka Producer with reliability settings
from confluent_kafka import Producer
import json
import logging

class ReliableKafkaProducer:
    def __init__(self, bootstrap_servers: str):
        self.producer = Producer({
            'bootstrap.servers': bootstrap_servers,

            # Reliability settings
            'acks': 'all',  # Wait for all replicas to acknowledge
            'enable.idempotence': True,  # Prevent duplicates on retry
            'max.in.flight.requests.per.connection': 5,  # With idempotence

            # Retry settings
            'retries': 2147483647,  # Retry indefinitely
            'retry.backoff.ms': 100,
            'delivery.timeout.ms': 120000,  # 2 minute timeout

            # Batching for throughput
            'batch.size': 16384,  # 16KB batches
            'linger.ms': 5,  # Wait up to 5ms to fill batch

            # Compression
            'compression.type': 'snappy',

            # Monitoring
            'statistics.interval.ms': 60000,
        })

    def send(self, topic: str, key: str, value: dict) -> None:
        """Send message with delivery confirmation."""
        try:
            self.producer.produce(
                topic=topic,
                key=key.encode('utf-8'),
                value=json.dumps(value).encode('utf-8'),
                callback=self._delivery_callback
            )
            # Flush ensures message is sent (blocking)
            # In production, call poll() periodically instead
            self.producer.poll(0)
        except BufferError:
            logging.error("Producer queue full, waiting...")
            self.producer.poll(1)  # Wait for space
            self.send(topic, key, value)  # Retry

    def _delivery_callback(self, err, msg):
        if err:
            logging.error(f"Delivery failed: {err}")
            # In production: implement dead letter queue
        else:
            logging.debug(f"Delivered to {msg.topic()}[{msg.partition()}]")

    def flush(self):
        """Wait for all messages to be delivered."""
        self.producer.flush()


# Usage
producer = ReliableKafkaProducer('kafka1:9092,kafka2:9092,kafka3:9092')
producer.send(
    topic='orders',
    key='order-12345',
    value={'order_id': '12345', 'amount': 99.99, 'status': 'created'}
)
                    </code>
                </div>

                <h4 class="mt-3">Kafka Consumer - At-Least-Once Processing</h4>
                <div class="code-block">
                    <code>
# Python - Kafka Consumer with manual commit
from confluent_kafka import Consumer, KafkaError
import json
import logging

class ReliableKafkaConsumer:
    def __init__(self, bootstrap_servers: str, group_id: str, topics: list):
        self.consumer = Consumer({
            'bootstrap.servers': bootstrap_servers,
            'group.id': group_id,

            # Manual offset management for at-least-once
            'enable.auto.commit': False,
            'auto.offset.reset': 'earliest',

            # Consumer settings
            'max.poll.interval.ms': 300000,  # 5 min max processing time
            'session.timeout.ms': 45000,
            'heartbeat.interval.ms': 15000,

            # Fetch settings
            'fetch.min.bytes': 1,
            'fetch.max.wait.ms': 500,
            'max.partition.fetch.bytes': 1048576,  # 1MB
        })
        self.consumer.subscribe(topics)
        self.running = True

    def process_messages(self, handler):
        """
        Process messages with at-least-once delivery.
        Handler must be idempotent (safe to process duplicates).
        """
        while self.running:
            msg = self.consumer.poll(timeout=1.0)

            if msg is None:
                continue

            if msg.error():
                if msg.error().code() == KafkaError._PARTITION_EOF:
                    continue
                else:
                    logging.error(f"Consumer error: {msg.error()}")
                    continue

            try:
                # Parse message
                key = msg.key().decode('utf-8') if msg.key() else None
                value = json.loads(msg.value().decode('utf-8'))

                # Process (must be idempotent!)
                handler(key, value)

                # Commit AFTER successful processing
                self.consumer.commit(msg)
                logging.debug(f"Committed offset {msg.offset()}")

            except json.JSONDecodeError as e:
                logging.error(f"Invalid JSON: {e}")
                # Commit to skip bad message (or send to DLQ)
                self.consumer.commit(msg)

            except Exception as e:
                logging.error(f"Processing failed: {e}")
                # DON'T commit - message will be redelivered
                # Implement backoff to prevent tight retry loop

    def close(self):
        self.running = False
        self.consumer.close()


# Usage with idempotent handler
def process_order(key: str, value: dict):
    order_id = value['order_id']

    # Idempotency check - have we processed this?
    if db.exists(f"processed:{order_id}"):
        logging.info(f"Order {order_id} already processed, skipping")
        return

    # Process the order
    db.save_order(value)

    # Mark as processed
    db.set(f"processed:{order_id}", "1", ttl=86400)


consumer = ReliableKafkaConsumer(
    bootstrap_servers='kafka1:9092,kafka2:9092',
    group_id='order-processor',
    topics=['orders']
)
consumer.process_messages(process_order)
                    </code>
                </div>

                <h4 class="mt-3">Go Implementation - High Performance Consumer</h4>
                <div class="code-block">
                    <code>
// Go - High-throughput Kafka consumer with worker pool

package main

import (
    "context"
    "encoding/json"
    "log"
    "sync"

    "github.com/segmentio/kafka-go"
)

type Message struct {
    OrderID string  `json:"order_id"`
    Amount  float64 `json:"amount"`
}

type KafkaConsumer struct {
    reader      *kafka.Reader
    workerCount int
    messages    chan kafka.Message
    wg          sync.WaitGroup
}

func NewKafkaConsumer(brokers []string, topic, groupID string, workers int) *KafkaConsumer {
    reader := kafka.NewReader(kafka.ReaderConfig{
        Brokers:        brokers,
        Topic:          topic,
        GroupID:        groupID,
        MinBytes:       1,
        MaxBytes:       10e6, // 10MB
        CommitInterval: 0,    // Manual commit
    })

    return &KafkaConsumer{
        reader:      reader,
        workerCount: workers,
        messages:    make(chan kafka.Message, workers*2),
    }
}

func (c *KafkaConsumer) Start(ctx context.Context, handler func(Message) error) {
    // Start worker pool
    for i := 0; i < c.workerCount; i++ {
        c.wg.Add(1)
        go c.worker(ctx, handler)
    }

    // Read messages
    for {
        select {
        case <-ctx.Done():
            close(c.messages)
            c.wg.Wait()
            return
        default:
            msg, err := c.reader.FetchMessage(ctx)
            if err != nil {
                log.Printf("Fetch error: %v", err)
                continue
            }
            c.messages <- msg
        }
    }
}

func (c *KafkaConsumer) worker(ctx context.Context, handler func(Message) error) {
    defer c.wg.Done()

    for msg := range c.messages {
        var m Message
        if err := json.Unmarshal(msg.Value, &m); err != nil {
            log.Printf("Unmarshal error: %v", err)
            c.commitMessage(ctx, msg) // Skip bad messages
            continue
        }

        if err := handler(m); err != nil {
            log.Printf("Handler error for %s: %v", m.OrderID, err)
            // Don't commit - will retry on restart
            continue
        }

        c.commitMessage(ctx, msg)
    }
}

func (c *KafkaConsumer) commitMessage(ctx context.Context, msg kafka.Message) {
    if err := c.reader.CommitMessages(ctx, msg); err != nil {
        log.Printf("Commit error: %v", err)
    }
}
                    </code>
                </div>
            </div>
        </div>

        <h2 class="mt-4">Failure Scenarios</h2>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>Kafka Broker Failures</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <div class="diagram-container">
                    <div class="mermaid">
sequenceDiagram
    participant P as Producer
    participant B1 as Broker 1 (Leader)
    participant B2 as Broker 2 (Follower)
    participant B3 as Broker 3 (Follower)
    participant ZK as Zookeeper

    P->>B1: Write message
    B1->>B2: Replicate
    B1->>B3: Replicate
    B2-->>B1: ACK
    B3-->>B1: ACK
    B1-->>P: ACK (acks=all)

    Note over B1: Broker 1 CRASHES

    B2->>ZK: Heartbeat missing
    ZK->>B2: You are new leader
    Note over B2: Broker 2 becomes leader
    P->>B2: Reconnect, continue writing
                    </div>
                </div>

                <h4>Recovery Configuration</h4>
                <div class="code-block">
                    <code>
# Kafka broker configuration for fast failover

# Replication factor (survive N-1 broker failures)
default.replication.factor=3

# Minimum replicas that must ACK before write is committed
min.insync.replicas=2

# With acks=all and min.insync.replicas=2:
# - Can survive 1 broker failure (2 of 3 still available)
# - Can't write if 2 brokers fail (only 1 < min.insync)

# Leader election settings
unclean.leader.election.enable=false  # Don't elect out-of-sync replicas

# Controller failover (if controller broker dies)
controller.socket.timeout.ms=30000
                    </code>
                </div>
            </div>
        </div>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>Consumer Failures and Rebalancing</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <h4>What Happens When a Consumer Dies</h4>
                <ol>
                    <li>Consumer stops sending heartbeats</li>
                    <li>After session.timeout.ms, broker marks consumer as dead</li>
                    <li>Consumer group coordinator triggers rebalance</li>
                    <li>Partitions reassigned to remaining consumers</li>
                    <li>Processing resumes from last committed offset</li>
                </ol>

                <div class="card mt-2" style="background: var(--error-bg);">
                    <strong>Problem:</strong> Messages between last commit and crash will be reprocessed (duplicates).
                    <br><br>
                    <strong>Solution:</strong> Make handlers idempotent using deduplication:
                </div>

                <div class="code-block">
                    <code>
# Python - Idempotent message processing

import hashlib
import redis

class IdempotentProcessor:
    def __init__(self, redis_client: redis.Redis, ttl_seconds: int = 86400):
        self.redis = redis_client
        self.ttl = ttl_seconds

    def process_once(self, message_id: str, handler, *args, **kwargs):
        """
        Execute handler only if message hasn't been processed before.
        Uses Redis for distributed deduplication.
        """
        dedup_key = f"processed:{message_id}"

        # Try to set the key (NX = only if not exists)
        was_set = self.redis.set(dedup_key, "1", nx=True, ex=self.ttl)

        if not was_set:
            # Already processed
            return {"status": "duplicate", "message_id": message_id}

        try:
            result = handler(*args, **kwargs)
            return {"status": "processed", "result": result}
        except Exception as e:
            # Delete the key so message can be retried
            self.redis.delete(dedup_key)
            raise


# Alternative: Use message content hash for deduplication
def get_message_id(message: dict) -> str:
    # Create deterministic ID from message content
    content = json.dumps(message, sort_keys=True)
    return hashlib.sha256(content.encode()).hexdigest()[:16]
                    </code>
                </div>
            </div>
        </div>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>Data Loss Scenarios and Prevention</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="background: var(--border-color);">
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Scenario</th>
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Data Loss Risk</th>
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Prevention</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Producer doesn't wait for ACK</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Message lost if broker crashes before replication</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Use acks=all</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">All replicas fail simultaneously</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Data loss if not on disk</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Spread replicas across racks/zones</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Consumer auto-commits before processing</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Message lost if consumer crashes mid-process</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Manual commit after processing</td>
                        </tr>
                        <tr>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Unclean leader election</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">Out-of-sync replica becomes leader, loses recent messages</td>
                            <td style="padding: 0.75rem; border: 1px solid var(--border-color);">unclean.leader.election.enable=false</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <h2 class="mt-4">Stream vs Batch Processing</h2>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>Processing Paradigms</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                    <thead>
                        <tr style="background: var(--border-color);">
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Aspect</th>
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Batch</th>
                            <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Stream</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Latency</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Minutes to hours</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Milliseconds to seconds</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Data</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Bounded (finite)</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Unbounded (infinite)</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Use case</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Reports, ETL, ML training</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Real-time analytics, alerts</td></tr>
                        <tr><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Tools</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Spark, Hadoop</td><td style="padding: 0.75rem; border: 1px solid var(--border-color);">Kafka Streams, Flink</td></tr>
                    </tbody>
                </table>

                <h4>Lambda Architecture</h4>
                <div class="diagram-container">
                    <div class="mermaid">
flowchart TB
    Data[Data] --> Batch[Batch Layer<br>Complete, accurate]
    Data --> Speed[Speed Layer<br>Low latency, approximate]
    Batch --> Serving[Serving Layer]
    Speed --> Serving
    Serving --> Query[Query]
                    </div>
                </div>
            </div>
        </div>

        <h2 class="mt-4">Distributed File Systems</h2>

        <div class="collapsible">
            <div class="collapsible-header">
                <span>GFS / HDFS Architecture</span>
                <span class="collapsible-icon">&#9660;</span>
            </div>
            <div class="collapsible-content">
                <div class="diagram-container">
                    <div class="mermaid">
flowchart TB
    Client[Client] --> Master[Master/NameNode]
    Master --> C1[Chunk Server 1]
    Master --> C2[Chunk Server 2]
    Master --> C3[Chunk Server 3]

    C1 <--> C2
    C2 <--> C3
                    </div>
                </div>

                <h4>Key Concepts</h4>
                <ul>
                    <li><strong>Chunks:</strong> Files split into large chunks (64-128 MB)</li>
                    <li><strong>Replication:</strong> Each chunk stored on multiple servers (typically 3)</li>
                    <li><strong>Master:</strong> Manages metadata, chunk locations</li>
                    <li><strong>Append-optimized:</strong> Designed for large sequential writes</li>
                </ul>
            </div>
        </div>

        <h2 class="mt-4">Self-Check Quiz</h2>
        <div class="quiz-container" id="module-quiz"></div>

            <div class="flex flex-between mt-4">
                <a href="module-03.html" class="btn btn-secondary">&larr; Previous Module</a>
                <button class="btn btn-primary" onclick="completeModule()">Mark Complete &rarr;</button>
            </div>
        </main>
    </div>

    <script src="../assets/js/app.js"></script>
    <script>
        // Sidebar toggle for mobile
        const sidebar = document.getElementById('sidebar');
        const sidebarToggle = document.getElementById('sidebarToggle');
        const sidebarOverlay = document.getElementById('sidebarOverlay');

        sidebarToggle.addEventListener('click', function() {
            sidebar.classList.toggle('open');
            sidebarOverlay.classList.toggle('open');
        });

        sidebarOverlay.addEventListener('click', function() {
            sidebar.classList.remove('open');
            sidebarOverlay.classList.remove('open');
        });

        // Update sidebar links based on completion status
        document.querySelectorAll('.sidebar-link[data-module]').forEach(link => {
            const moduleNum = parseInt(link.dataset.module);
            if (StaffEngPrep.ProgressTracker.isModuleComplete('systemDesign', moduleNum)) {
                link.classList.add('completed');
            }
        });

        document.addEventListener('DOMContentLoaded', function() {
            const quizQuestions = [
                {
                    question: "What is the main advantage of using Kafka over traditional queues?",
                    options: ["Simpler API", "Log-based with replay capability", "Lower cost", "Better security"],
                    correct: 1,
                    explanation: "Kafka's log-based design allows consumers to replay messages and track their own offsets."
                },
                {
                    question: "When should you choose stream processing over batch?",
                    options: ["When accuracy is critical", "When latency must be low", "When data is finite", "When simplicity is priority"],
                    correct: 1,
                    explanation: "Stream processing is ideal when you need low-latency, real-time results rather than waiting for batch jobs."
                }
            ];
            const quiz = new StaffEngPrep.Quiz('module-quiz', quizQuestions);
            quiz.render();
        });
        function completeModule() {
            StaffEngPrep.ProgressTracker.markModuleComplete('systemDesign', 4);
            alert('Module 4 marked as complete!');
            window.location.href = 'module-05.html';
        }

        // ============================================
        // Interactive Kafka Demo
        // ============================================
        let kafkaMessageId = 0;
        let partitionOffsets = [0, 0, 0];
        let consumerOffsets = [0, 0, 0];
        let partitionMessages = [[], [], []];
        let totalProduced = 0;
        let totalConsumed = 0;

        function addKafkaLog(message, type = '') {
            const log = document.getElementById('kafka-log');
            const entry = document.createElement('div');
            entry.className = `kafka-log-entry ${type}`;
            const timestamp = new Date().toLocaleTimeString();
            entry.textContent = `[${timestamp}] ${message}`;
            log.insertBefore(entry, log.firstChild);

            while (log.children.length > 15) {
                log.removeChild(log.lastChild);
            }
        }

        function updateKafkaStats() {
            document.getElementById('total-produced').textContent = totalProduced;
            document.getElementById('total-consumed').textContent = totalConsumed;
            document.getElementById('consumer-lag').textContent = totalProduced - totalConsumed;

            const balance = partitionMessages.map(p => p.length).join('/');
            document.getElementById('partition-balance').textContent = balance;

            for (let i = 0; i < 3; i++) {
                document.getElementById(`p${i}-offset`).textContent = `Offset: ${partitionOffsets[i]}`;
            }
        }

        function renderPartitionMessages(partitionId) {
            const container = document.getElementById(`messages-${partitionId}`);
            container.innerHTML = '';

            partitionMessages[partitionId].forEach((msg, idx) => {
                const msgEl = document.createElement('div');
                msgEl.className = `kafka-message ${idx < consumerOffsets[partitionId] ? 'committed' : 'uncommitted'}`;
                msgEl.textContent = msg.id;
                msgEl.title = `Key: ${msg.key}, Offset: ${idx}`;
                container.appendChild(msgEl);
            });
        }

        function produceMessage() {
            kafkaMessageId++;
            const key = Math.floor(Math.random() * 100);
            const partition = key % 3;

            // Animate producer
            const producer = document.getElementById('producer-1');
            producer.classList.add('sending');
            setTimeout(() => producer.classList.remove('sending'), 300);

            // Add message to partition
            setTimeout(() => {
                const message = {
                    id: kafkaMessageId,
                    key: key,
                    offset: partitionOffsets[partition]
                };

                partitionMessages[partition].push(message);
                partitionOffsets[partition]++;
                totalProduced++;

                renderPartitionMessages(partition);
                updateKafkaStats();

                addKafkaLog(`Produced msg ${kafkaMessageId} (key=${key}) â†’ Partition ${partition}`, 'produce');

                // Highlight new message
                const container = document.getElementById(`messages-${partition}`);
                const lastMsg = container.lastElementChild;
                if (lastMsg) {
                    lastMsg.classList.add('new');
                    setTimeout(() => lastMsg.classList.remove('new'), 500);
                }
            }, 150);
        }

        function consumeMessage() {
            // Find partition with unconsumed messages
            let partitionToConsume = -1;
            let minOffset = Infinity;

            for (let i = 0; i < 3; i++) {
                if (consumerOffsets[i] < partitionMessages[i].length) {
                    if (consumerOffsets[i] < minOffset) {
                        minOffset = consumerOffsets[i];
                        partitionToConsume = i;
                    }
                }
            }

            if (partitionToConsume === -1) {
                addKafkaLog('No messages to consume - consumer caught up!', '');
                return;
            }

            // Animate consumer
            const consumer = document.getElementById('consumer-1');
            consumer.classList.add('consuming');
            setTimeout(() => consumer.classList.remove('consuming'), 300);

            // Consume message
            setTimeout(() => {
                const msg = partitionMessages[partitionToConsume][consumerOffsets[partitionToConsume]];
                consumerOffsets[partitionToConsume]++;
                totalConsumed++;

                // Animate message being consumed
                const container = document.getElementById(`messages-${partitionToConsume}`);
                const msgEl = container.children[consumerOffsets[partitionToConsume] - 1];
                if (msgEl) {
                    msgEl.classList.add('consuming');
                    setTimeout(() => {
                        msgEl.classList.remove('consuming');
                        msgEl.classList.remove('uncommitted');
                        msgEl.classList.add('committed');
                    }, 300);
                }

                updateKafkaStats();
                addKafkaLog(`Consumed msg ${msg.id} from Partition ${partitionToConsume} (offset ${consumerOffsets[partitionToConsume] - 1})`, 'consume');

                // Commit offset
                setTimeout(() => {
                    addKafkaLog(`Committed offset ${consumerOffsets[partitionToConsume]} for Partition ${partitionToConsume}`, 'commit');
                }, 200);
            }, 150);
        }

        function produceBatch() {
            let count = 0;
            const batchSize = 5;

            const interval = setInterval(() => {
                produceMessage();
                count++;
                if (count >= batchSize) {
                    clearInterval(interval);
                    addKafkaLog(`Batch complete: ${batchSize} messages produced`, '');
                }
            }, 200);
        }

        // Initialize with some messages
        document.addEventListener('DOMContentLoaded', function() {
            // Add a few initial messages
            for (let i = 0; i < 3; i++) {
                setTimeout(() => produceMessage(), i * 100);
            }
        });
    </script>
</body>
</html>
