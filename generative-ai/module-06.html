<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Module 6: Training Massive Models at Scale - Staff Engineer Prep</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        /* Module-specific styles */
        .scale-card {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 1rem;
            padding: 1.5rem;
            margin: 1rem 0;
            color: white;
        }

        .scale-number {
            font-size: 2.5rem;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .scale-label {
            color: #a0aec0;
            font-size: 0.9rem;
            margin-top: 0.25rem;
        }

        .gpu-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(60px, 1fr));
            gap: 0.5rem;
            padding: 1rem;
            background: #0f0f1a;
            border-radius: 0.75rem;
            margin: 1rem 0;
        }

        .gpu-chip {
            aspect-ratio: 1;
            background: linear-gradient(135deg, #2d3748 0%, #1a202c 100%);
            border-radius: 0.5rem;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7rem;
            color: #a0aec0;
            transition: all 0.3s ease;
            border: 2px solid transparent;
        }

        .gpu-chip.active {
            background: linear-gradient(135deg, #48bb78 0%, #38a169 100%);
            color: white;
            border-color: #68d391;
            box-shadow: 0 0 15px rgba(72, 187, 120, 0.4);
        }

        .gpu-chip.data-parallel {
            background: linear-gradient(135deg, #4299e1 0%, #3182ce 100%);
            color: white;
        }

        .gpu-chip.model-parallel {
            background: linear-gradient(135deg, #ed8936 0%, #dd6b20 100%);
            color: white;
        }

        .gpu-chip.pipeline-parallel {
            background: linear-gradient(135deg, #9f7aea 0%, #805ad5 100%);
            color: white;
        }

        .parallel-demo {
            background: linear-gradient(135deg, #0f3460 0%, #16213e 100%);
            border-radius: 1rem;
            padding: 2rem;
            margin: 1.5rem 0;
        }

        .parallel-controls {
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            justify-content: center;
            margin-bottom: 1.5rem;
        }

        .parallel-btn {
            padding: 0.75rem 1.5rem;
            border: none;
            border-radius: 0.5rem;
            cursor: pointer;
            font-weight: 600;
            color: white;
            transition: all 0.3s ease;
        }

        .parallel-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.3);
        }

        .parallel-btn.data { background: linear-gradient(135deg, #4299e1 0%, #3182ce 100%); }
        .parallel-btn.model { background: linear-gradient(135deg, #ed8936 0%, #dd6b20 100%); }
        .parallel-btn.pipeline { background: linear-gradient(135deg, #9f7aea 0%, #805ad5 100%); }
        .parallel-btn.tensor { background: linear-gradient(135deg, #f56565 0%, #e53e3e 100%); }

        .parallel-btn.active {
            box-shadow: 0 0 20px rgba(255, 255, 255, 0.3);
            transform: scale(1.05);
        }

        .parallel-explanation {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 0.5rem;
            padding: 1rem;
            color: white;
            margin-top: 1rem;
        }

        .precision-bar {
            display: flex;
            align-items: center;
            margin: 0.75rem 0;
        }

        .precision-label {
            width: 80px;
            font-family: 'Fira Code', monospace;
            font-size: 0.85rem;
            color: #a0aec0;
        }

        .precision-visual {
            flex: 1;
            display: flex;
            gap: 2px;
            height: 30px;
        }

        .bit {
            flex: 1;
            background: rgba(255, 255, 255, 0.2);
            border-radius: 2px;
            transition: all 0.3s ease;
        }

        .bit.sign { background: linear-gradient(135deg, #f56565 0%, #e53e3e 100%); }
        .bit.exponent { background: linear-gradient(135deg, #4299e1 0%, #3182ce 100%); }
        .bit.mantissa { background: linear-gradient(135deg, #48bb78 0%, #38a169 100%); }

        .precision-info {
            width: 120px;
            text-align: right;
            font-size: 0.85rem;
            color: #fcd34d;
        }

        .rlhf-flow {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            padding: 1.5rem;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 1rem;
            margin: 1rem 0;
        }

        .rlhf-step {
            display: flex;
            align-items: center;
            gap: 1rem;
            padding: 1rem;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 0.75rem;
            transition: all 0.3s ease;
        }

        .rlhf-step:hover {
            background: rgba(255, 255, 255, 0.1);
            transform: translateX(5px);
        }

        .rlhf-number {
            width: 40px;
            height: 40px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            color: white;
            flex-shrink: 0;
        }

        .rlhf-step:nth-child(1) .rlhf-number { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
        .rlhf-step:nth-child(2) .rlhf-number { background: linear-gradient(135deg, #48bb78 0%, #38a169 100%); }
        .rlhf-step:nth-child(3) .rlhf-number { background: linear-gradient(135deg, #ed8936 0%, #dd6b20 100%); }

        .rlhf-content h4 {
            color: white;
            margin-bottom: 0.25rem;
        }

        .rlhf-content p {
            color: #a0aec0;
            font-size: 0.9rem;
            margin: 0;
        }

        .infra-card {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1rem 0;
            transition: all 0.3s ease;
        }

        .infra-card:hover {
            border-color: var(--primary-color);
            box-shadow: 0 5px 20px rgba(0, 0, 0, 0.1);
        }

        .infra-icon {
            font-size: 2rem;
            margin-bottom: 0.5rem;
        }

        .memory-timeline {
            display: flex;
            flex-direction: column;
            gap: 0.5rem;
            padding: 1rem;
            background: #1a1a2e;
            border-radius: 0.75rem;
            margin: 1rem 0;
        }

        .memory-layer {
            display: flex;
            align-items: center;
            gap: 1rem;
        }

        .memory-label {
            width: 100px;
            font-size: 0.85rem;
            color: #a0aec0;
        }

        .memory-bar {
            flex: 1;
            height: 30px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 4px;
            overflow: hidden;
            position: relative;
        }

        .memory-fill {
            height: 100%;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: flex-end;
            padding-right: 0.5rem;
            font-size: 0.75rem;
            color: white;
            font-weight: 600;
        }

        .memory-fill.activations {
            background: linear-gradient(90deg, #f56565 0%, #fc8181 100%);
        }

        .memory-fill.gradients {
            background: linear-gradient(90deg, #4299e1 0%, #63b3ed 100%);
        }

        .memory-fill.optimizer {
            background: linear-gradient(90deg, #9f7aea 0%, #b794f4 100%);
        }

        .memory-fill.params {
            background: linear-gradient(90deg, #48bb78 0%, #68d391 100%);
        }

        .recall-question {
            background: var(--card-bg);
            border: 2px solid var(--border-color);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1rem 0;
        }

        .recall-question.revealed .recall-answer {
            display: block;
        }

        .recall-answer {
            display: none;
            margin-top: 1rem;
            padding-top: 1rem;
            border-top: 1px solid var(--border-color);
            color: var(--secondary-color);
        }

        .reveal-btn {
            margin-top: 1rem;
            padding: 0.5rem 1rem;
            background: var(--primary-color);
            color: white;
            border: none;
            border-radius: 0.5rem;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="nav-container">
            <a href="../index.html" class="logo">StaffEngPrep</a>
            <ul class="nav-links">
                <li><a href="../coding-rounds/index.html">Coding</a></li>
                <li><a href="../system-design/index.html">System Design</a></li>
                <li><a href="../company-specific/index.html">Companies</a></li>
                <li><a href="../behavioral/index.html">Behavioral</a></li>
                <li><a href="index.html" style="color: var(--primary-color);">Gen AI</a></li>
            </ul>
        </div>
    </nav>

    <div class="layout-with-sidebar">
        <aside class="sidebar" id="sidebar">
            <nav class="sidebar-nav">
                <div class="sidebar-section">
                    <div class="sidebar-section-title">Getting Started</div>
                    <a href="index.html" class="sidebar-link">Introduction</a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Foundations</div>
                    <a href="module-01.html" class="sidebar-link" data-module="1">
                        <span class="sidebar-link-number">1</span>Setup + Core Math
                    </a>
                    <a href="module-02.html" class="sidebar-link" data-module="2">
                        <span class="sidebar-link-number">2</span>Terminology + MNIST
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">LLM Deep Dive</div>
                    <a href="module-03.html" class="sidebar-link" data-module="3">
                        <span class="sidebar-link-number">3</span>LLM Basics
                    </a>
                    <a href="module-04.html" class="sidebar-link" data-module="4">
                        <span class="sidebar-link-number">4</span>Attention Mechanisms
                    </a>
                    <a href="module-05.html" class="sidebar-link" data-module="5">
                        <span class="sidebar-link-number">5</span>LLM Coding: GPT
                    </a>
                    <a href="module-06.html" class="sidebar-link active" data-module="6">
                        <span class="sidebar-link-number">6</span>Training at Scale
                    </a>
                    <a href="module-07.html" class="sidebar-link" data-module="7">
                        <span class="sidebar-link-number">7</span>Optimization Hacks
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">RAG & Retrieval</div>
                    <a href="module-08.html" class="sidebar-link" data-module="8">
                        <span class="sidebar-link-number">8</span>RAG Fundamentals
                    </a>
                    <a href="module-09.html" class="sidebar-link" data-module="9">
                        <span class="sidebar-link-number">9</span>RAG Implementation
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Agents & Systems</div>
                    <a href="module-10.html" class="sidebar-link" data-module="10">
                        <span class="sidebar-link-number">10</span>AI Agents
                    </a>
                    <a href="module-11.html" class="sidebar-link" data-module="11">
                        <span class="sidebar-link-number">11</span>Context Engineering
                    </a>
                    <a href="module-12.html" class="sidebar-link" data-module="12">
                        <span class="sidebar-link-number">12</span>AI Engineering
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Advanced Topics</div>
                    <a href="module-13.html" class="sidebar-link" data-module="13">
                        <span class="sidebar-link-number">13</span>Thinking Models
                    </a>
                    <a href="module-14.html" class="sidebar-link" data-module="14">
                        <span class="sidebar-link-number">14</span>Multi-modal Models
                    </a>
                </div>

                <div class="sidebar-section">
                    <div class="sidebar-section-title">Capstone & Career</div>
                    <a href="module-15.html" class="sidebar-link" data-module="15">
                        <span class="sidebar-link-number">15</span>Capstone Project
                    </a>
                    <a href="module-16.html" class="sidebar-link" data-module="16">
                        <span class="sidebar-link-number">16</span>Career Goals
                    </a>
                </div>
            </nav>
        </aside>

        <button class="sidebar-toggle" id="sidebarToggle">&#9776;</button>
        <div class="sidebar-overlay" id="sidebarOverlay"></div>

        <main class="main-content">
            <h1>Module 6: Think Like an Engineer - Training Massive Models</h1>

            <div class="card mt-3">
                <h3>What You Will Learn</h3>
                <ul>
                    <li>Understand the scale of modern LLM training (compute, data, cost)</li>
                    <li>Master distributed training strategies: data, model, pipeline, tensor parallelism</li>
                    <li>Learn mixed precision training and why FP16/BF16 matter</li>
                    <li>Understand gradient checkpointing for memory efficiency</li>
                    <li>Grasp RLHF, PPO, DPO, and how models are aligned</li>
                    <li>Know the infrastructure required for training at scale</li>
                </ul>
            </div>

            <!-- Section 1: Scale of Modern LLM Training -->
            <h2 class="mt-4">1. The Scale of Modern LLM Training</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Power Plant Analogy</h4>
                    <p>Training GPT-4 is like running a small power plant. You need:</p>
                    <ul>
                        <li><strong>Massive compute:</strong> Thousands of GPUs running 24/7 for months</li>
                        <li><strong>Enormous data:</strong> Trillions of tokens (essentially the internet)</li>
                        <li><strong>Complex orchestration:</strong> Keeping thousands of machines in sync</li>
                        <li><strong>Reliability engineering:</strong> Handling failures without losing progress</li>
                    </ul>

                    <h4>The Numbers That Matter</h4>
                    <div class="card-grid">
                        <div class="scale-card">
                            <div class="scale-number">175B</div>
                            <div class="scale-label">GPT-3 Parameters</div>
                        </div>
                        <div class="scale-card">
                            <div class="scale-number">~1.8T</div>
                            <div class="scale-label">GPT-4 Estimated Parameters (MoE)</div>
                        </div>
                        <div class="scale-card">
                            <div class="scale-number">~$100M</div>
                            <div class="scale-label">GPT-4 Training Cost (estimated)</div>
                        </div>
                        <div class="scale-card">
                            <div class="scale-number">25,000+</div>
                            <div class="scale-label">A100 GPUs for GPT-4</div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Core Concepts: Training Compute</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>FLOPS: The Currency of AI Training</h4>
                    <p><strong>FLOPS</strong> = Floating Point Operations Per Second. Training compute is measured in total FLOPs required.</p>

                    <h4>Chinchilla Scaling Laws</h4>
                    <p>DeepMind's research showed optimal training requires balancing model size and data:</p>

                    <div class="code-block">
                        <code><pre># Chinchilla optimal: 20 tokens per parameter
# For a 70B parameter model:
optimal_tokens = 70e9 * 20  # = 1.4 trillion tokens

# Training compute (approximate):
# C ≈ 6 * N * D
# where N = parameters, D = tokens
compute_flops = 6 * 70e9 * 1.4e12  # ≈ 5.9e23 FLOPs</pre></code>
                    </div>

                    <h4>Model Comparison</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Model</th>
                                <th>Parameters</th>
                                <th>Training Tokens</th>
                                <th>Estimated FLOPs</th>
                                <th>Estimated Cost</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>GPT-3</td>
                                <td>175B</td>
                                <td>300B</td>
                                <td>3.1e23</td>
                                <td>~$5M</td>
                            </tr>
                            <tr>
                                <td>LLaMA 2 70B</td>
                                <td>70B</td>
                                <td>2T</td>
                                <td>8.4e23</td>
                                <td>~$10M</td>
                            </tr>
                            <tr>
                                <td>Claude 3</td>
                                <td>Unknown</td>
                                <td>Unknown</td>
                                <td>~1e25 (est.)</td>
                                <td>~$50M+ (est.)</td>
                            </tr>
                            <tr>
                                <td>GPT-4</td>
                                <td>~1.8T (MoE)</td>
                                <td>~13T (est.)</td>
                                <td>~2e25 (est.)</td>
                                <td>~$100M (est.)</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="key-insight">
                        <p><strong>Key Insight:</strong> Training compute has grown 10x every year since 2010. GPT-4 required approximately 10,000x more compute than GPT-2. This exponential growth drives the need for distributed training techniques.</p>
                    </div>
                </div>
            </div>

            <!-- Section 2: Data Parallelism -->
            <h2 class="mt-4">2. Data Parallelism: Same Model, Different Data</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Assembly Line Analogy</h4>
                    <p>Imagine you have one recipe (the model) and 8 chefs (GPUs). Each chef makes the same dish with different ingredients (data). At the end, they share notes (gradients) and all update their recipes identically.</p>

                    <div class="parallel-demo">
                        <h4 style="color: white; text-align: center;">Interactive: Data Parallelism Visualization</h4>
                        <div class="gpu-grid" id="data-parallel-grid">
                            <div class="gpu-chip data-parallel">GPU 0<br>Batch 0</div>
                            <div class="gpu-chip data-parallel">GPU 1<br>Batch 1</div>
                            <div class="gpu-chip data-parallel">GPU 2<br>Batch 2</div>
                            <div class="gpu-chip data-parallel">GPU 3<br>Batch 3</div>
                            <div class="gpu-chip data-parallel">GPU 4<br>Batch 4</div>
                            <div class="gpu-chip data-parallel">GPU 5<br>Batch 5</div>
                            <div class="gpu-chip data-parallel">GPU 6<br>Batch 6</div>
                            <div class="gpu-chip data-parallel">GPU 7<br>Batch 7</div>
                        </div>
                        <div class="parallel-explanation">
                            <strong>Data Parallelism:</strong> Each GPU has a complete copy of the model but processes different data. After forward/backward pass, gradients are synchronized (AllReduce) and all GPUs update identically.
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Core Concepts</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>How Data Parallelism Works</h4>
                    <ol>
                        <li><strong>Replicate:</strong> Copy the model to all GPUs</li>
                        <li><strong>Distribute:</strong> Split the batch across GPUs</li>
                        <li><strong>Forward:</strong> Each GPU processes its data independently</li>
                        <li><strong>Backward:</strong> Each GPU computes gradients independently</li>
                        <li><strong>Synchronize:</strong> AllReduce averages gradients across GPUs</li>
                        <li><strong>Update:</strong> All GPUs apply the same gradient update</li>
                    </ol>

                    <h4>Effective Batch Size</h4>
                    <div class="code-block">
                        <code>effective_batch_size = per_gpu_batch_size * num_gpus * gradient_accumulation_steps</code>
                    </div>

                    <p>Example: 8 GPUs * 32 per-GPU batch * 4 accumulation steps = 1024 effective batch size</p>

                    <h4>Types of Data Parallelism</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Type</th>
                                <th>Description</th>
                                <th>Memory Efficiency</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>DP</strong> (DataParallel)</td>
                                <td>PyTorch's simple wrapper</td>
                                <td>Poor (GPU 0 bottleneck)</td>
                                <td>Single-machine, quick experiments</td>
                            </tr>
                            <tr>
                                <td><strong>DDP</strong> (DistributedDataParallel)</td>
                                <td>Efficient multi-process</td>
                                <td>Good</td>
                                <td>Production training</td>
                            </tr>
                            <tr>
                                <td><strong>FSDP</strong> (Fully Sharded)</td>
                                <td>Shards model across GPUs</td>
                                <td>Excellent</td>
                                <td>Large models</td>
                            </tr>
                            <tr>
                                <td><strong>ZeRO</strong> (DeepSpeed)</td>
                                <td>Progressive sharding stages</td>
                                <td>Excellent</td>
                                <td>Very large models</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Code Walkthrough: DDP and FSDP</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>Distributed Data Parallel (DDP)</h4>
                    <div class="code-block">
                        <code><pre>import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_ddp(rank, world_size):
    """Initialize distributed training."""
    dist.init_process_group(
        backend='nccl',  # Use NCCL for GPU training
        init_method='env://',  # Get config from environment
        world_size=world_size,
        rank=rank
    )
    torch.cuda.set_device(rank)

def train_with_ddp(rank, world_size, model, train_dataset):
    """Train with Distributed Data Parallel."""
    setup_ddp(rank, world_size)

    # Move model to GPU and wrap with DDP
    model = model.to(rank)
    model = DDP(model, device_ids=[rank])

    # Create distributed sampler
    sampler = torch.utils.data.distributed.DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )

    loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=32,
        sampler=sampler,
        num_workers=4,
        pin_memory=True
    )

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        sampler.set_epoch(epoch)  # Important for shuffling!

        for batch in loader:
            inputs, targets = batch
            inputs = inputs.to(rank)
            targets = targets.to(rank)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)
            loss.backward()  # Gradients auto-synchronized!
            optimizer.step()

    dist.destroy_process_group()

# Launch: torchrun --nproc_per_node=8 train.py</pre></code>
                    </div>

                    <h4>Fully Sharded Data Parallel (FSDP)</h4>
                    <div class="code-block">
                        <code><pre>from torch.distributed.fsdp import (
    FullyShardedDataParallel as FSDP,
    MixedPrecision,
    ShardingStrategy,
)
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
import functools

def train_with_fsdp(rank, world_size, model):
    """
    FSDP: Shards model parameters, gradients, and optimizer states.

    Memory savings:
    - Parameters: 1/N of original (sharded)
    - Gradients: 1/N of original (sharded)
    - Optimizer: 1/N of original (sharded)

    This allows training models that don't fit on a single GPU!
    """
    setup_ddp(rank, world_size)

    # Define what layers to wrap
    auto_wrap_policy = functools.partial(
        transformer_auto_wrap_policy,
        transformer_layer_cls={TransformerBlock}  # Your block class
    )

    # Mixed precision config
    mixed_precision = MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
    )

    # Wrap model with FSDP
    model = FSDP(
        model,
        auto_wrap_policy=auto_wrap_policy,
        mixed_precision=mixed_precision,
        sharding_strategy=ShardingStrategy.FULL_SHARD,  # Maximum sharding
        device_id=rank,
    )

    # Training loop is similar to DDP
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for batch in loader:
        optimizer.zero_grad()
        loss = model(batch)
        loss.backward()
        optimizer.step()

    # Save checkpoint (FSDP requires special handling)
    with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT):
        state_dict = model.state_dict()
        if rank == 0:
            torch.save(state_dict, 'checkpoint.pt')</pre></code>
                    </div>

                    <div class="callout callout-info">
                        <div class="callout-title">FSDP vs ZeRO</div>
                        <p>FSDP is PyTorch's native implementation of ZeRO-3 (DeepSpeed). ZeRO has three stages:</p>
                        <ul>
                            <li><strong>ZeRO-1:</strong> Shard optimizer states only</li>
                            <li><strong>ZeRO-2:</strong> + Shard gradients</li>
                            <li><strong>ZeRO-3:</strong> + Shard parameters (= FSDP)</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Section 3: Model Parallelism -->
            <h2 class="mt-4">3. Model Parallelism: Splitting the Model Across GPUs</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Assembly Stages Analogy</h4>
                    <p>Instead of 8 chefs each making a complete dish, imagine a single dish that requires 8 specialized stations. Station 1 does prep, Station 2 does cooking, Station 3 does plating, etc. The dish moves through the pipeline.</p>

                    <h4>Why Model Parallelism?</h4>
                    <p>When a model is too large to fit on a single GPU, you must split it. A 175B parameter model in FP16 requires:</p>

                    <div class="code-block">
                        <code>Memory = 175B params * 2 bytes (FP16) = 350 GB</code>
                    </div>

                    <p>An A100 has 80GB memory. You need at least 5 GPUs just for the parameters, plus more for activations and gradients.</p>

                    <div class="diagram-container">
                        <div class="mermaid">
graph LR
    subgraph "Model Parallelism"
        subgraph "GPU 0"
            A[Layers 0-7]
        end
        subgraph "GPU 1"
            B[Layers 8-15]
        end
        subgraph "GPU 2"
            C[Layers 16-23]
        end
        subgraph "GPU 3"
            D[Layers 24-31]
        end
    end
    A --> B --> C --> D
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Types of Model Parallelism</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>1. Pipeline Parallelism (PP)</h4>
                    <p>Split model by layers. Each GPU holds consecutive layers.</p>

                    <ul>
                        <li><strong>Pros:</strong> Straightforward implementation, works with any architecture</li>
                        <li><strong>Cons:</strong> GPU bubbles (idle time), high memory for activations</li>
                    </ul>

                    <div class="diagram-container">
                        <div class="mermaid">
gantt
    title Pipeline Parallelism - Micro-batch Scheduling
    dateFormat X
    axisFormat %s

    section GPU 0
    F0-MB0 :a1, 0, 1
    F0-MB1 :a2, 1, 2
    B0-MB0 :a3, 5, 6
    B0-MB1 :a4, 6, 7

    section GPU 1
    F1-MB0 :b1, 1, 2
    F1-MB1 :b2, 2, 3
    B1-MB0 :b3, 4, 5
    B1-MB1 :b4, 5, 6

    section GPU 2
    F2-MB0 :c1, 2, 3
    F2-MB1 :c2, 3, 4
    B2-MB0 :c3, 3, 4
    B2-MB1 :c4, 4, 5
                        </div>
                    </div>
                    <p class="text-muted text-center">F = Forward pass, B = Backward pass, MB = Micro-batch. Notice the "bubble" (idle time) at start and end.</p>

                    <h4>2. Tensor Parallelism (TP)</h4>
                    <p>Split individual operations (matrix multiplications) across GPUs.</p>

                    <div class="code-block">
                        <code><pre># Example: Column-parallel linear layer
# Original: Y = XW where W is [d_in, d_out]
# Split W into [W1, W2, W3, W4] where each Wi is [d_in, d_out/4]

# GPU 0: Y0 = X @ W0  # [batch, d_out/4]
# GPU 1: Y1 = X @ W1  # [batch, d_out/4]
# GPU 2: Y2 = X @ W2  # [batch, d_out/4]
# GPU 3: Y3 = X @ W3  # [batch, d_out/4]

# AllGather: Y = concat([Y0, Y1, Y2, Y3])  # [batch, d_out]</pre></code>
                    </div>

                    <ul>
                        <li><strong>Pros:</strong> Better GPU utilization, lower bubble</li>
                        <li><strong>Cons:</strong> Requires architectural changes, high communication</li>
                    </ul>

                    <h4>Comparison</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>Data Parallel</th>
                                <th>Pipeline Parallel</th>
                                <th>Tensor Parallel</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>What's split?</td>
                                <td>Data (batches)</td>
                                <td>Model (by layers)</td>
                                <td>Model (within layers)</td>
                            </tr>
                            <tr>
                                <td>Communication</td>
                                <td>Gradient sync</td>
                                <td>Activations between stages</td>
                                <td>Per-operation AllReduce/AllGather</td>
                            </tr>
                            <tr>
                                <td>GPU utilization</td>
                                <td>High</td>
                                <td>Has bubbles</td>
                                <td>High (within node)</td>
                            </tr>
                            <tr>
                                <td>Best for</td>
                                <td>Model fits on 1 GPU</td>
                                <td>Model too large, across nodes</td>
                                <td>Large models within a node</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>3D Parallelism: Combining All Three</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>How Megatron-LM Does It</h4>
                    <p>NVIDIA's Megatron-LM combines all three parallelism types:</p>

                    <div class="diagram-container">
                        <div class="mermaid">
graph TB
    subgraph "3D Parallelism: 64 GPUs"
        subgraph "Data Parallel Group 1"
            subgraph "Pipeline Stage 1"
                A1[TP: GPU 0-3]
            end
            subgraph "Pipeline Stage 2"
                A2[TP: GPU 4-7]
            end
        end
        subgraph "Data Parallel Group 2"
            subgraph "Pipeline Stage 1 "
                B1[TP: GPU 8-11]
            end
            subgraph "Pipeline Stage 2 "
                B2[TP: GPU 12-15]
            end
        end
    end

    A1 --> A2
    B1 --> B2
                        </div>
                    </div>

                    <p>Configuration example for 64 GPUs:</p>
                    <ul>
                        <li><strong>Tensor Parallel:</strong> 4 GPUs (within a single node)</li>
                        <li><strong>Pipeline Parallel:</strong> 4 stages (across nodes)</li>
                        <li><strong>Data Parallel:</strong> 4 replicas (across node groups)</li>
                        <li>Total: 4 * 4 * 4 = 64 GPUs</li>
                    </ul>

                    <div class="code-block">
                        <code><pre># Megatron-LM style config
# python train.py \
#     --tensor-model-parallel-size 4 \
#     --pipeline-model-parallel-size 4 \
#     --data-parallel-size 4 \
#     --num-layers 96 \
#     --hidden-size 12288 \
#     --num-attention-heads 96</pre></code>
                    </div>

                    <div class="key-insight">
                        <p><strong>Rule of Thumb:</strong> Use tensor parallelism within a node (fast NVLink), pipeline parallelism across nodes (slower network), and data parallelism to scale further.</p>
                    </div>
                </div>
            </div>

            <!-- Section 4: Mixed Precision Training -->
            <h2 class="mt-4">4. Mixed Precision Training: FP16, BF16, FP32</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Precision Spectrum</h4>
                    <p>Think of numerical precision like a ruler's tick marks. More precision = finer tick marks, but also more expensive to store and compute.</p>

                    <div style="background: #1a1a2e; padding: 1.5rem; border-radius: 0.75rem; margin: 1rem 0;">
                        <h5 style="color: white; margin-bottom: 1rem;">Floating Point Formats</h5>

                        <div class="precision-bar">
                            <span class="precision-label" style="color: white;">FP32</span>
                            <div class="precision-visual">
                                <div class="bit sign" title="Sign (1 bit)"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div>
                            </div>
                            <span class="precision-info">32 bits, 4 bytes</span>
                        </div>

                        <div class="precision-bar">
                            <span class="precision-label" style="color: white;">FP16</span>
                            <div class="precision-visual">
                                <div class="bit sign"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div>
                            </div>
                            <span class="precision-info">16 bits, 2 bytes</span>
                        </div>

                        <div class="precision-bar">
                            <span class="precision-label" style="color: white;">BF16</span>
                            <div class="precision-visual">
                                <div class="bit sign"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit exponent"></div><div class="bit exponent"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div><div class="bit mantissa"></div><div class="bit mantissa"></div>
                                <div class="bit mantissa"></div>
                            </div>
                            <span class="precision-info">16 bits, 2 bytes</span>
                        </div>

                        <div style="display: flex; gap: 1rem; margin-top: 1rem; justify-content: center;">
                            <span style="display: flex; align-items: center; gap: 0.5rem;"><div style="width: 20px; height: 20px; background: linear-gradient(135deg, #f56565, #e53e3e); border-radius: 4px;"></div><span style="color: #a0aec0; font-size: 0.8rem;">Sign</span></span>
                            <span style="display: flex; align-items: center; gap: 0.5rem;"><div style="width: 20px; height: 20px; background: linear-gradient(135deg, #4299e1, #3182ce); border-radius: 4px;"></div><span style="color: #a0aec0; font-size: 0.8rem;">Exponent</span></span>
                            <span style="display: flex; align-items: center; gap: 0.5rem;"><div style="width: 20px; height: 20px; background: linear-gradient(135deg, #48bb78, #38a169); border-radius: 4px;"></div><span style="color: #a0aec0; font-size: 0.8rem;">Mantissa</span></span>
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Core Concepts</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>Why Mixed Precision?</h4>
                    <ul>
                        <li><strong>2x memory savings:</strong> Store weights in FP16/BF16 instead of FP32</li>
                        <li><strong>2-8x faster compute:</strong> Modern GPUs have FP16/BF16 tensor cores</li>
                        <li><strong>Same accuracy:</strong> With proper techniques, no degradation</li>
                    </ul>

                    <h4>FP16 vs BF16</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>FP16</th>
                                <th>BF16</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Exponent bits</td>
                                <td>5</td>
                                <td>8 (same as FP32!)</td>
                            </tr>
                            <tr>
                                <td>Mantissa bits</td>
                                <td>10</td>
                                <td>7</td>
                            </tr>
                            <tr>
                                <td>Range</td>
                                <td>Limited (overflows easily)</td>
                                <td>Same as FP32</td>
                            </tr>
                            <tr>
                                <td>Precision</td>
                                <td>Higher</td>
                                <td>Lower</td>
                            </tr>
                            <tr>
                                <td>Requires loss scaling?</td>
                                <td>Yes</td>
                                <td>No</td>
                            </tr>
                            <tr>
                                <td>Hardware support</td>
                                <td>NVIDIA V100+</td>
                                <td>A100+, TPUs</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="key-insight">
                        <p><strong>Industry Standard:</strong> Most modern LLM training uses BF16 because it has the same range as FP32 (no overflow issues) while being half the size. FP16 requires careful loss scaling to avoid gradient underflow.</p>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Code Walkthrough: Mixed Precision Training</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <div class="code-block">
                        <code><pre>import torch
from torch.cuda.amp import autocast, GradScaler

def train_mixed_precision(model, train_loader, optimizer, device='cuda'):
    """
    Mixed precision training with automatic loss scaling (for FP16).

    The GradScaler:
    1. Scales up the loss before backward (prevents gradient underflow)
    2. Unscales gradients before optimizer step
    3. Skips updates if gradients contain inf/nan
    4. Adjusts scale factor dynamically
    """
    scaler = GradScaler()  # For FP16. Not needed for BF16.

    model.train()
    for batch in train_loader:
        inputs, targets = batch
        inputs = inputs.to(device)
        targets = targets.to(device)

        optimizer.zero_grad()

        # Forward pass in mixed precision
        with autocast(dtype=torch.float16):  # or torch.bfloat16
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)

        # Backward pass with scaled gradients
        scaler.scale(loss).backward()

        # Unscale and clip gradients
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # Optimizer step with dynamic scaling
        scaler.step(optimizer)
        scaler.update()

# BF16 is simpler - no scaler needed!
def train_bf16(model, train_loader, optimizer, device='cuda'):
    """BF16 training - simpler because no loss scaling needed."""
    model.train()
    for batch in train_loader:
        inputs, targets = batch
        inputs = inputs.to(device)
        targets = targets.to(device)

        optimizer.zero_grad()

        with autocast(dtype=torch.bfloat16):
            outputs = model(inputs)
            loss = loss_fn(outputs, targets)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()</pre></code>
                    </div>

                    <div class="callout callout-warning">
                        <div class="callout-title">What to Keep in FP32</div>
                        <p>Some operations should stay in FP32 for numerical stability:</p>
                        <ul>
                            <li>Loss computation</li>
                            <li>Softmax (for attention)</li>
                            <li>Layer normalization</li>
                            <li>Optimizer states (always FP32)</li>
                        </ul>
                        <p>PyTorch's autocast handles this automatically for most cases.</p>
                    </div>
                </div>
            </div>

            <!-- Section 5: Gradient Checkpointing -->
            <h2 class="mt-4">5. Gradient Checkpointing: Trading Compute for Memory</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Save Point Analogy</h4>
                    <p>In a video game, you can either save your progress constantly (uses lots of storage) or only at checkpoints (uses less storage but you might need to replay sections). Gradient checkpointing is the same tradeoff:</p>

                    <ul>
                        <li><strong>No checkpointing:</strong> Save all activations (high memory, fast backward)</li>
                        <li><strong>Full checkpointing:</strong> Save nothing, recompute everything (low memory, slow backward)</li>
                        <li><strong>Selective checkpointing:</strong> Save at intervals (balanced)</li>
                    </ul>

                    <h4>Memory Breakdown During Training</h4>
                    <div class="memory-timeline">
                        <div class="memory-layer">
                            <span class="memory-label">Parameters</span>
                            <div class="memory-bar">
                                <div class="memory-fill params" style="width: 15%;">15%</div>
                            </div>
                        </div>
                        <div class="memory-layer">
                            <span class="memory-label">Gradients</span>
                            <div class="memory-bar">
                                <div class="memory-fill gradients" style="width: 15%;">15%</div>
                            </div>
                        </div>
                        <div class="memory-layer">
                            <span class="memory-label">Optimizer</span>
                            <div class="memory-bar">
                                <div class="memory-fill optimizer" style="width: 30%;">30% (AdamW: 2x params)</div>
                            </div>
                        </div>
                        <div class="memory-layer">
                            <span class="memory-label">Activations</span>
                            <div class="memory-bar">
                                <div class="memory-fill activations" style="width: 40%;">40% (scales with batch/seq)</div>
                            </div>
                        </div>
                    </div>
                    <p class="text-muted text-center">Activations often dominate memory, especially for long sequences. Checkpointing targets this.</p>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Code Walkthrough</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <div class="code-block">
                        <code><pre>from torch.utils.checkpoint import checkpoint, checkpoint_sequential

class TransformerBlockWithCheckpoint(nn.Module):
    """Transformer block with optional gradient checkpointing."""

    def __init__(self, config, use_checkpoint=False):
        super().__init__()
        self.use_checkpoint = use_checkpoint
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(self, x):
        if self.use_checkpoint and self.training:
            # Checkpoint this block - don't save activations
            # They'll be recomputed during backward pass
            return checkpoint(self._forward, x, use_reentrant=False)
        return self._forward(x)

    def _forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class GPTWithCheckpointing(nn.Module):
    """GPT with selective checkpointing."""

    def __init__(self, config, checkpoint_every=2):
        super().__init__()
        self.blocks = nn.ModuleList([
            TransformerBlockWithCheckpoint(
                config,
                use_checkpoint=(i % checkpoint_every == 0)
            )
            for i in range(config.n_layer)
        ])
        # ... rest of GPT implementation

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        return x


# Alternative: Use checkpoint_sequential for contiguous blocks
def forward_with_sequential_checkpoint(self, x):
    """Checkpoint every N layers as a group."""
    # Split blocks into groups of 4
    chunk_size = 4
    for i in range(0, len(self.blocks), chunk_size):
        chunk = self.blocks[i:i+chunk_size]
        x = checkpoint_sequential(chunk, 1, x, use_reentrant=False)
    return x</pre></code>
                    </div>

                    <h4>Memory vs Compute Tradeoff</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Strategy</th>
                                <th>Memory Savings</th>
                                <th>Compute Overhead</th>
                                <th>Use Case</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>No checkpointing</td>
                                <td>0%</td>
                                <td>0%</td>
                                <td>Small models, lots of GPU memory</td>
                            </tr>
                            <tr>
                                <td>Every 2nd layer</td>
                                <td>~40%</td>
                                <td>~25%</td>
                                <td>Moderate memory pressure</td>
                            </tr>
                            <tr>
                                <td>Every layer</td>
                                <td>~60%</td>
                                <td>~33%</td>
                                <td>High memory pressure</td>
                            </tr>
                            <tr>
                                <td>Full (sqrt(N) checkpoints)</td>
                                <td>~70%</td>
                                <td>~50%</td>
                                <td>Very large models</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="interview-tip">
                        <p><strong>Interview Tip:</strong> "When would you NOT use gradient checkpointing?" Answer: When you have excess GPU memory and training time is the bottleneck. Checkpointing trades compute for memory - if memory is not the constraint, it just slows you down.</p>
                    </div>
                </div>
            </div>

            <!-- Section 6: RLHF -->
            <h2 class="mt-4">6. RLHF: Reward Models, PPO, and DPO</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Mental Models</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Teaching Analogy</h4>
                    <p>Imagine teaching a student (the LLM) to write essays:</p>
                    <ol>
                        <li><strong>Pretraining:</strong> The student reads millions of essays to learn language</li>
                        <li><strong>Supervised Fine-tuning (SFT):</strong> The student practices with example essays and feedback</li>
                        <li><strong>RLHF:</strong> A teacher (reward model) grades essays, and the student optimizes for higher grades</li>
                    </ol>

                    <div class="rlhf-flow">
                        <div class="rlhf-step">
                            <div class="rlhf-number">1</div>
                            <div class="rlhf-content">
                                <h4>Supervised Fine-tuning (SFT)</h4>
                                <p>Train on high-quality human-written responses. The model learns the format and style of helpful responses.</p>
                            </div>
                        </div>
                        <div class="rlhf-step">
                            <div class="rlhf-number">2</div>
                            <div class="rlhf-content">
                                <h4>Reward Model Training</h4>
                                <p>Humans rank model outputs (A > B). Train a model to predict these preferences. This becomes the "grading function."</p>
                            </div>
                        </div>
                        <div class="rlhf-step">
                            <div class="rlhf-number">3</div>
                            <div class="rlhf-content">
                                <h4>RL Optimization (PPO/DPO)</h4>
                                <p>Fine-tune the policy (LLM) to maximize reward while staying close to the SFT model (KL penalty).</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Core Concepts: The RLHF Pipeline</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>Step 1: Collect Human Preferences</h4>
                    <p>Given a prompt, generate multiple responses and have humans rank them:</p>

                    <div class="code-block">
                        <code><pre>Prompt: "Explain quantum computing to a 5-year-old"

Response A: "Quantum computing uses qubits which can be in superposition..."
Response B: "Imagine you have a magic coin that can be heads AND tails..."
Response C: "Quantum computing leverages quantum mechanical phenomena..."

Human ranking: B > A > C</pre></code>
                    </div>

                    <h4>Step 2: Train Reward Model</h4>
                    <p>Train a model to predict human preferences using the Bradley-Terry model:</p>

                    <div class="code-block">
                        <code><pre>import torch
import torch.nn as nn

class RewardModel(nn.Module):
    """
    Reward model: Given a (prompt, response) pair, output a scalar reward.

    Architecture: Same as LLM but with a value head instead of LM head.
    """
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model  # Pretrained LLM backbone
        self.value_head = nn.Linear(base_model.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        # Get hidden states from base model
        outputs = self.base_model(input_ids, attention_mask=attention_mask)
        hidden_states = outputs.last_hidden_state

        # Use last token's hidden state for reward
        # (Could also use mean pooling)
        last_hidden = hidden_states[:, -1, :]
        reward = self.value_head(last_hidden)
        return reward


def compute_preference_loss(reward_model, chosen_ids, rejected_ids):
    """
    Bradley-Terry loss: model should assign higher reward to chosen response.

    P(chosen > rejected) = sigmoid(r_chosen - r_rejected)
    Loss = -log(P(chosen > rejected))
    """
    r_chosen = reward_model(chosen_ids)
    r_rejected = reward_model(rejected_ids)

    # Log-sigmoid of the difference
    loss = -torch.log(torch.sigmoid(r_chosen - r_rejected)).mean()
    return loss</pre></code>
                    </div>

                    <h4>Step 3: PPO Optimization</h4>
                    <p>Use the reward model to fine-tune the LLM with Proximal Policy Optimization:</p>

                    <div class="code-block">
                        <code><pre>def ppo_step(policy_model, ref_model, reward_model, prompts, kl_coef=0.1):
    """
    One step of PPO optimization.

    Key insight: We want high reward but also want to stay close to the
    reference model (prevents reward hacking and catastrophic forgetting).

    Objective: maximize E[reward - kl_coef * KL(policy || reference)]
    """
    # Generate responses from current policy
    with torch.no_grad():
        responses = policy_model.generate(prompts, max_length=256)
        rewards = reward_model(responses)
        ref_logprobs = ref_model.log_probs(responses)

    # Get policy log probs (with gradients)
    policy_logprobs = policy_model.log_probs(responses)

    # KL divergence penalty
    kl_div = policy_logprobs - ref_logprobs

    # PPO objective (simplified)
    # Full PPO has clipping, advantage estimation, etc.
    advantages = rewards - kl_coef * kl_div

    # Policy gradient loss
    loss = -(policy_logprobs * advantages.detach()).mean()

    return loss</pre></code>
                    </div>

                    <div class="diagram-container">
                        <div class="mermaid">
graph LR
    subgraph "RLHF Training Loop"
        A[Prompt] --> B[Policy Model]
        B --> C[Generate Response]
        C --> D[Reward Model]
        D --> E[Compute Reward]
        E --> F[PPO Update]
        F --> B

        G[Reference Model] --> H[KL Penalty]
        B --> H
        H --> F
    end
                        </div>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>DPO: Direct Preference Optimization</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>Why DPO?</h4>
                    <p>PPO is complex and unstable. DPO simplifies RLHF by showing that the optimal policy can be obtained directly without training a separate reward model.</p>

                    <div class="key-insight">
                        <p><strong>Key Insight:</strong> DPO collapses the reward modeling and RL steps into a single supervised learning objective. The reward is implicit in the preference data itself.</p>
                    </div>

                    <h4>DPO Loss</h4>
                    <div class="code-block">
                        <code><pre>def dpo_loss(policy_model, ref_model, chosen_ids, rejected_ids, beta=0.1):
    """
    Direct Preference Optimization loss.

    Instead of: train reward model -> run PPO
    DPO does: directly optimize policy on preference data

    The math shows this is equivalent to RLHF with KL-constrained reward!
    """
    # Get log probs from policy and reference
    policy_chosen_logps = policy_model.log_probs(chosen_ids)
    policy_rejected_logps = policy_model.log_probs(rejected_ids)
    ref_chosen_logps = ref_model.log_probs(chosen_ids)
    ref_rejected_logps = ref_model.log_probs(rejected_ids)

    # Log ratios
    chosen_ratio = policy_chosen_logps - ref_chosen_logps
    rejected_ratio = policy_rejected_logps - ref_rejected_logps

    # DPO loss (equivalent to RLHF with implicit reward)
    loss = -torch.log(
        torch.sigmoid(beta * (chosen_ratio - rejected_ratio))
    ).mean()

    return loss</pre></code>
                    </div>

                    <h4>PPO vs DPO</h4>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Aspect</th>
                                <th>PPO (RLHF)</th>
                                <th>DPO</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Separate reward model?</td>
                                <td>Yes, trained separately</td>
                                <td>No, implicit in loss</td>
                            </tr>
                            <tr>
                                <td>Online generation?</td>
                                <td>Yes, samples during training</td>
                                <td>No, uses fixed preference data</td>
                            </tr>
                            <tr>
                                <td>Complexity</td>
                                <td>High (4 models in memory)</td>
                                <td>Low (2 models)</td>
                            </tr>
                            <tr>
                                <td>Stability</td>
                                <td>Can be unstable</td>
                                <td>More stable</td>
                            </tr>
                            <tr>
                                <td>Used by</td>
                                <td>ChatGPT, Claude (originally)</td>
                                <td>LLaMA 2, Zephyr, many open models</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- Section 7: Constitutional AI -->
            <h2 class="mt-4">7. Constitutional AI and Alignment Techniques</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Constitutional AI (CAI)</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>The Self-Critique Loop</h4>
                    <p>Anthropic's Constitutional AI teaches models to critique and revise their own outputs according to a set of principles (the "constitution").</p>

                    <div class="diagram-container">
                        <div class="mermaid">
graph TB
    subgraph "Constitutional AI Process"
        A[Prompt] --> B[Initial Response]
        B --> C[Self-Critique]
        C --> D["Critique based on principles:<br/>- Be helpful<br/>- Be harmless<br/>- Be honest"]
        D --> E[Revised Response]
        E --> F[Preference Pair: Revised > Initial]
        F --> G[Train with DPO/RLHF]
    end
                        </div>
                    </div>

                    <h4>Example Constitution Principles</h4>
                    <div class="code-block">
                        <code><pre>CONSTITUTION = [
    "Please choose the response that is most helpful, harmless, and honest.",
    "Please choose the response that is less harmful or toxic.",
    "Please choose the response that is more respectful of privacy.",
    "Please choose the response that does not encourage illegal activity.",
    "Please choose the response that is more age-appropriate.",
]

def generate_constitutional_preference(model, prompt):
    """Generate a preference pair using self-critique."""

    # Generate initial response
    initial = model.generate(prompt)

    # Self-critique
    critique_prompt = f"""
    Human: {prompt}
    Assistant: {initial}

    Critique: Please identify any ways this response could be more helpful,
    harmless, or honest according to these principles: {CONSTITUTION}
    """
    critique = model.generate(critique_prompt)

    # Revise based on critique
    revise_prompt = f"""
    Human: {prompt}
    Assistant: {initial}
    Critique: {critique}

    Please write a revised response that addresses the critique:
    """
    revised = model.generate(revise_prompt)

    # Return as preference pair
    return {
        "prompt": prompt,
        "chosen": revised,
        "rejected": initial
    }</pre></code>
                    </div>

                    <h4>Benefits of Constitutional AI</h4>
                    <ul>
                        <li><strong>Scalable:</strong> Reduces need for human labelers</li>
                        <li><strong>Transparent:</strong> Principles are explicit and auditable</li>
                        <li><strong>Iterative:</strong> Can apply multiple rounds of critique</li>
                        <li><strong>Flexible:</strong> Easy to update principles as needed</li>
                    </ul>
                </div>
            </div>

            <!-- Section 8: Infrastructure -->
            <h2 class="mt-4">8. Infrastructure: GPU Clusters, Networking, Storage</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>The Hardware Stack</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <div class="card-grid">
                        <div class="infra-card">
                            <div class="infra-icon">&#x1F4BB;</div>
                            <h4>GPUs</h4>
                            <ul>
                                <li><strong>NVIDIA A100:</strong> 80GB HBM2e, 312 TFLOPS FP16</li>
                                <li><strong>NVIDIA H100:</strong> 80GB HBM3, 989 TFLOPS FP16</li>
                                <li><strong>NVIDIA B200:</strong> Next-gen, ~2x H100</li>
                            </ul>
                            <p class="text-muted">Cost: ~$30K-$40K per GPU</p>
                        </div>

                        <div class="infra-card">
                            <div class="infra-icon">&#x1F517;</div>
                            <h4>Networking</h4>
                            <ul>
                                <li><strong>NVLink:</strong> 600 GB/s within node (8 GPUs)</li>
                                <li><strong>InfiniBand:</strong> 400 Gb/s between nodes</li>
                                <li><strong>RDMA:</strong> Direct memory access, low latency</li>
                            </ul>
                            <p class="text-muted">Network is often the bottleneck for model parallelism</p>
                        </div>

                        <div class="infra-card">
                            <div class="infra-icon">&#x1F4BE;</div>
                            <h4>Storage</h4>
                            <ul>
                                <li><strong>NVMe SSDs:</strong> Fast checkpoint saves</li>
                                <li><strong>Distributed FS:</strong> Lustre, GPFS, or cloud storage</li>
                                <li><strong>Data loading:</strong> Often CPU-bound, needs optimization</li>
                            </ul>
                            <p class="text-muted">Training data: 10-100 TB typical</p>
                        </div>

                        <div class="infra-card">
                            <div class="infra-icon">&#x26A1;</div>
                            <h4>Power & Cooling</h4>
                            <ul>
                                <li><strong>Per GPU:</strong> 400-700W (H100)</li>
                                <li><strong>Cluster:</strong> 10-50 MW for large training</li>
                                <li><strong>PUE:</strong> 1.1-1.5 for efficient datacenters</li>
                            </ul>
                            <p class="text-muted">Electricity cost: $1-5M per training run</p>
                        </div>
                    </div>

                    <h4>Typical Cluster Configuration</h4>
                    <div class="code-block">
                        <code><pre># GPT-4 scale training (estimated):
# - 25,000+ A100 GPUs
# - 3,125 nodes (8 GPUs per node)
# - NVSwitch within node, InfiniBand between nodes
# - Training time: ~3-6 months

# LLaMA 2 70B (published):
# - 2,000 A100 80GB GPUs
# - 250 nodes
# - Training time: ~1.7M GPU-hours

# Typical setup for training a 7B model:
# - 8-64 A100 GPUs (1-8 nodes)
# - NVLink within node, Ethernet/IB between nodes
# - Training time: 1-4 weeks</pre></code>
                    </div>
                </div>
            </div>

            <div class="collapsible">
                <div class="collapsible-header">
                    <span>Fault Tolerance and Checkpointing</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <h4>Why Fault Tolerance Matters</h4>
                    <p>With thousands of GPUs running for months, hardware failures are inevitable:</p>

                    <ul>
                        <li>GPU memory errors</li>
                        <li>Network connectivity issues</li>
                        <li>Power outages</li>
                        <li>Software bugs (NaN gradients, OOM)</li>
                    </ul>

                    <p>At 25,000 GPUs, you might see multiple failures per day. Without proper checkpointing, you could lose days of training.</p>

                    <h4>Checkpointing Strategy</h4>
                    <div class="code-block">
                        <code><pre>def save_checkpoint(model, optimizer, scheduler, step, path):
    """
    Save distributed checkpoint with FSDP/DeepSpeed.

    Key considerations:
    1. Async saving (don't block training)
    2. Efficient serialization (FSDP shards)
    3. Redundant storage (multiple locations)
    4. Validation (checksum, test load)
    """
    checkpoint = {
        'step': step,
        'model_state_dict': model.state_dict(),  # FSDP handles sharding
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'rng_state': torch.get_rng_state(),
        'cuda_rng_state': torch.cuda.get_rng_state(),
    }

    # Save to distributed filesystem
    if dist.get_rank() == 0:
        torch.save(checkpoint, path)

    # Sync to ensure checkpoint is complete before continuing
    dist.barrier()


# Checkpoint schedule:
# - Every N steps (e.g., every 1000 steps)
# - On validation improvement
# - On failure detection (if possible)
# - Keep last K checkpoints (rolling)</pre></code>
                    </div>

                    <div class="key-insight">
                        <p><strong>Industry Practice:</strong> Large training runs use elastic training frameworks like PyTorch Elastic or DeepSpeed that can automatically recover from failures, restart failed workers, and resume from checkpoints without manual intervention.</p>
                    </div>
                </div>
            </div>

            <!-- Active Recall Questions -->
            <h2 class="mt-4">9. Active Recall Questions</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Test Your Understanding</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q1:</strong> What are the three components sharded in ZeRO-3/FSDP?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p>ZeRO-3 (and FSDP) shard three components across GPUs:</p>
                            <ol>
                                <li><strong>Optimizer states</strong> (ZeRO-1): Momentum and variance for Adam</li>
                                <li><strong>Gradients</strong> (ZeRO-2): Gradient tensors during backward pass</li>
                                <li><strong>Parameters</strong> (ZeRO-3): The model weights themselves</li>
                            </ol>
                            <p>Each stage reduces memory by 1/N where N is the number of GPUs.</p>
                        </div>
                    </div>

                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q2:</strong> Why is BF16 preferred over FP16 for LLM training?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p>BF16 has the same exponent range as FP32 (8 bits vs FP16's 5 bits). This means:</p>
                            <ul>
                                <li>No overflow/underflow issues during training</li>
                                <li>No need for loss scaling</li>
                                <li>Simpler training code</li>
                            </ul>
                            <p>BF16 trades precision (7 mantissa bits vs 10) for range, which works well for deep learning where range matters more than precision.</p>
                        </div>
                    </div>

                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q3:</strong> What is the role of the KL penalty in RLHF?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p>The KL penalty (KL(policy || reference)) serves two purposes:</p>
                            <ol>
                                <li><strong>Prevents reward hacking:</strong> Stops the model from finding degenerate solutions that game the reward model</li>
                                <li><strong>Prevents catastrophic forgetting:</strong> Keeps the model close to its pretrained capabilities</li>
                            </ol>
                            <p>Without KL penalty, the model might produce nonsensical outputs that happen to score high on a flawed reward model.</p>
                        </div>
                    </div>

                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q4:</strong> When would you use tensor parallelism vs pipeline parallelism?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p><strong>Tensor Parallelism:</strong></p>
                            <ul>
                                <li>Within a single node (fast NVLink connectivity)</li>
                                <li>Splits individual matrix operations across GPUs</li>
                                <li>High communication but high utilization</li>
                            </ul>
                            <p><strong>Pipeline Parallelism:</strong></p>
                            <ul>
                                <li>Across nodes (slower network connectivity)</li>
                                <li>Splits model by layers</li>
                                <li>Lower communication but has bubble overhead</li>
                            </ul>
                            <p>Rule of thumb: TP within node, PP across nodes.</p>
                        </div>
                    </div>

                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q5:</strong> How does DPO simplify RLHF?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p>DPO removes the need for:</p>
                            <ol>
                                <li>A separate reward model</li>
                                <li>Online generation during training</li>
                                <li>Complex RL algorithms like PPO</li>
                            </ol>
                            <p>Instead, DPO directly optimizes the policy using preference pairs with a closed-form loss. The math shows this is equivalent to RLHF with the optimal reward function.</p>
                            <p>DPO requires only 2 models (policy and reference) vs 4 for PPO (policy, reference, reward, value).</p>
                        </div>
                    </div>

                    <div class="recall-question" onclick="this.classList.toggle('revealed')">
                        <strong>Q6:</strong> What is the Chinchilla scaling law and why does it matter?
                        <button class="reveal-btn">Reveal Answer</button>
                        <div class="recall-answer">
                            <p>The Chinchilla scaling law states that for compute-optimal training:</p>
                            <div class="code-block" style="background: var(--code-bg);">
                                <code>Optimal tokens ≈ 20 * parameters</code>
                            </div>
                            <p>This means a 70B model should be trained on ~1.4T tokens for optimal efficiency. Training on fewer tokens wastes model capacity; training on more wastes compute.</p>
                            <p>This changed how labs think about scaling: sometimes a smaller model trained on more data beats a larger undertrained model.</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Mini Project -->
            <h2 class="mt-4">10. Mini Project: Implement DPO Training</h2>

            <div class="collapsible open">
                <div class="collapsible-header">
                    <span>Project Instructions</span>
                    <span class="collapsible-icon">&#9660;</span>
                </div>
                <div class="collapsible-content">
                    <div class="card" style="background: linear-gradient(135deg, rgba(139, 92, 246, 0.1) 0%, rgba(236, 72, 153, 0.1) 100%); border: 2px solid rgba(139, 92, 246, 0.3);">
                        <h4>Build a DPO Training Pipeline</h4>
                        <p>Implement Direct Preference Optimization to align a small language model.</p>

                        <h5>Tasks:</h5>
                        <ol>
                            <li>Load a small pretrained model (GPT-2 small or similar)</li>
                            <li>Create/load preference data (chosen vs rejected pairs)</li>
                            <li>Implement the DPO loss function</li>
                            <li>Train for a few epochs and evaluate</li>
                            <li>Compare generations before and after DPO</li>
                        </ol>

                        <h5>Starter Code:</h5>
                    </div>

                    <div class="code-block">
                        <code><pre>import torch
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

def get_log_probs(model, input_ids, attention_mask):
    """Get per-token log probabilities."""
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits[:, :-1, :]  # Exclude last position
        labels = input_ids[:, 1:]  # Shift labels

        log_probs = F.log_softmax(logits, dim=-1)
        token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(-1)).squeeze(-1)

        # Mask padding
        mask = attention_mask[:, 1:].float()
        return (token_log_probs * mask).sum(-1) / mask.sum(-1)

def dpo_loss(policy_model, ref_model, chosen_ids, chosen_mask,
             rejected_ids, rejected_mask, beta=0.1):
    """
    Compute Direct Preference Optimization (DPO) loss.

    DPO directly optimizes the policy to prefer chosen over rejected responses
    without needing a separate reward model. The key insight is that the optimal
    policy under RLHF can be expressed in closed form, allowing us to reparameterize
    the reward function in terms of the policy itself.

    The DPO loss is derived from:
        r(x, y) = beta * log(pi(y|x) / pi_ref(y|x)) + beta * log Z(x)

    This leads to the loss:
        L_DPO = -E[log sigmoid(beta * (log_ratio_w - log_ratio_l))]

    where:
        - log_ratio_w = log pi(y_w|x) - log pi_ref(y_w|x)  # chosen (winner)
        - log_ratio_l = log pi(y_l|x) - log pi_ref(y_l|x)  # rejected (loser)

    Args:
        policy_model: Model being trained (pi_theta)
        ref_model: Frozen reference model (pi_ref), typically the SFT model
        chosen_ids: Token IDs for preferred responses (y_w)
        chosen_mask: Attention mask for chosen responses
        rejected_ids: Token IDs for dispreferred responses (y_l)
        rejected_mask: Attention mask for rejected responses
        beta: Temperature parameter controlling deviation from reference (default 0.1)
               Higher beta = stronger KL constraint, more conservative updates
               Lower beta = allows more deviation from reference model

    Returns:
        loss: Scalar DPO loss (to be minimized)
        metrics: Dict with useful training metrics for logging
    """
    # Step 1: Get log probabilities from the policy model (being trained)
    # We need gradients here since we're training this model
    policy_chosen_logps = get_log_probs_trainable(policy_model, chosen_ids, chosen_mask)
    policy_rejected_logps = get_log_probs_trainable(policy_model, rejected_ids, rejected_mask)

    # Step 2: Get log probabilities from the reference model (frozen)
    # No gradients needed - this is our anchor point
    with torch.no_grad():
        ref_chosen_logps = get_log_probs(ref_model, chosen_ids, chosen_mask)
        ref_rejected_logps = get_log_probs(ref_model, rejected_ids, rejected_mask)

    # Step 3: Compute log ratios (how much policy differs from reference)
    # Positive ratio means policy assigns higher probability than reference
    chosen_log_ratios = policy_chosen_logps - ref_chosen_logps
    rejected_log_ratios = policy_rejected_logps - ref_rejected_logps

    # Step 4: Compute the DPO loss
    # We want: chosen_log_ratio > rejected_log_ratio (prefer chosen)
    # The sigmoid pushes this difference through a smooth threshold
    logits = beta * (chosen_log_ratios - rejected_log_ratios)

    # Binary cross-entropy loss: -log(sigmoid(x)) for positive class
    # This encourages the model to increase the margin between chosen and rejected
    loss = -F.logsigmoid(logits).mean()

    # Step 5: Compute metrics for monitoring training health
    with torch.no_grad():
        # Accuracy: how often does the model prefer chosen over rejected?
        accuracy = (logits > 0).float().mean()

        # Reward margin: implicit reward difference (higher is better)
        reward_margin = (chosen_log_ratios - rejected_log_ratios).mean()

        # KL divergence from reference (regularization check)
        chosen_kl = (policy_chosen_logps - ref_chosen_logps).mean()
        rejected_kl = (policy_rejected_logps - ref_rejected_logps).mean()

        metrics = {
            "loss": loss.item(),
            "accuracy": accuracy.item(),
            "reward_margin": reward_margin.item(),
            "chosen_kl": chosen_kl.item(),
            "rejected_kl": rejected_kl.item(),
            "chosen_logps": policy_chosen_logps.mean().item(),
            "rejected_logps": policy_rejected_logps.mean().item(),
        }

    return loss, metrics


def get_log_probs_trainable(model, input_ids, attention_mask):
    """
    Get per-token log probabilities WITH gradients for training.
    Unlike get_log_probs(), this version allows backpropagation.
    """
    outputs = model(input_ids, attention_mask=attention_mask)
    logits = outputs.logits[:, :-1, :]  # Exclude last position
    labels = input_ids[:, 1:]  # Shift labels (predict next token)

    log_probs = F.log_softmax(logits, dim=-1)
    token_log_probs = torch.gather(log_probs, 2, labels.unsqueeze(-1)).squeeze(-1)

    # Mask padding tokens
    mask = attention_mask[:, 1:].float()
    # Return mean log prob per sequence (normalized by length)
    return (token_log_probs * mask).sum(-1) / mask.sum(-1)

# Load models
model_name = "gpt2"
policy_model = AutoModelForCausalLM.from_pretrained(model_name)
ref_model = AutoModelForCausalLM.from_pretrained(model_name)
ref_model.eval()  # Freeze reference model

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Example preference data - in production, you'd have thousands of these
# Each example has a prompt, a preferred (chosen) response, and a dispreferred (rejected) response
preferences = [
    {
        "prompt": "Write a haiku about coding:",
        "chosen": "Lines of code flow free\nBugs arise then fade away\nPrograms come alive",
        "rejected": "code is good i like code very much",
    },
    {
        "prompt": "Explain recursion briefly:",
        "chosen": "Recursion is when a function calls itself to solve smaller subproblems. It needs a base case to stop.",
        "rejected": "its when things repeat over and over basically loops",
    },
    {
        "prompt": "What is machine learning?",
        "chosen": "Machine learning enables computers to learn patterns from data without being explicitly programmed, improving through experience.",
        "rejected": "computers learning stuff from data i guess",
    },
]


# ============================================================================
# DPO TRAINING LOOP
# ============================================================================

def prepare_dpo_batch(preferences, tokenizer, device):
    """
    Tokenize preference pairs for DPO training.

    For each preference example, we create:
    - chosen: prompt + preferred response
    - rejected: prompt + dispreferred response

    Both are tokenized and padded to the same length for batching.
    """
    chosen_texts = []
    rejected_texts = []

    for pref in preferences:
        # Concatenate prompt with each response
        chosen_texts.append(pref["prompt"] + " " + pref["chosen"])
        rejected_texts.append(pref["prompt"] + " " + pref["rejected"])

    # Tokenize with padding and truncation
    chosen_encodings = tokenizer(
        chosen_texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

    rejected_encodings = tokenizer(
        rejected_texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

    return {
        "chosen_ids": chosen_encodings["input_ids"].to(device),
        "chosen_mask": chosen_encodings["attention_mask"].to(device),
        "rejected_ids": rejected_encodings["input_ids"].to(device),
        "rejected_mask": rejected_encodings["attention_mask"].to(device),
    }


def generate_sample(model, tokenizer, prompt, max_length=100):
    """Generate a sample response from the model for evaluation."""
    model.eval()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def train_dpo(
    policy_model,
    ref_model,
    preferences,
    tokenizer,
    num_epochs=3,
    learning_rate=1e-5,
    beta=0.1,
    device="cuda" if torch.cuda.is_available() else "cpu",
):
    """
    Main DPO training loop.

    This implements the complete training pipeline:
    1. Prepare data batches
    2. Forward pass through both policy and reference models
    3. Compute DPO loss
    4. Backpropagate and update policy model
    5. Log metrics and generate samples for evaluation

    Args:
        policy_model: Model to train
        ref_model: Frozen reference model (anchor for KL regularization)
        preferences: List of preference dictionaries
        tokenizer: Tokenizer for the models
        num_epochs: Number of training epochs
        learning_rate: Learning rate for AdamW optimizer
        beta: DPO temperature (higher = more conservative)
        device: Training device (cuda/cpu)
    """
    # Move models to device
    policy_model = policy_model.to(device)
    ref_model = ref_model.to(device)

    # Freeze reference model - we never update it
    for param in ref_model.parameters():
        param.requires_grad = False

    # Use AdamW optimizer with weight decay for regularization
    optimizer = torch.optim.AdamW(
        policy_model.parameters(),
        lr=learning_rate,
        weight_decay=0.01,
    )

    # Learning rate scheduler for stable training
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=num_epochs,
        eta_min=learning_rate * 0.1,
    )

    # Prepare the batch (in production, you'd use a DataLoader)
    batch = prepare_dpo_batch(preferences, tokenizer, device)

    # Store metrics for plotting
    all_metrics = []

    # Sample prompt for generating comparisons before/after training
    test_prompt = "Write a haiku about coding:"

    print("=" * 60)
    print("DPO TRAINING STARTED")
    print("=" * 60)
    print(f"Device: {device}")
    print(f"Epochs: {num_epochs}")
    print(f"Learning rate: {learning_rate}")
    print(f"Beta (KL coefficient): {beta}")
    print(f"Number of preference pairs: {len(preferences)}")
    print("=" * 60)

    # Generate sample BEFORE training
    print("\n--- BEFORE TRAINING ---")
    print(f"Prompt: {test_prompt}")
    print(f"Response: {generate_sample(policy_model, tokenizer, test_prompt)}")

    # Training loop
    for epoch in range(num_epochs):
        policy_model.train()

        # Forward pass: compute DPO loss
        loss, metrics = dpo_loss(
            policy_model=policy_model,
            ref_model=ref_model,
            chosen_ids=batch["chosen_ids"],
            chosen_mask=batch["chosen_mask"],
            rejected_ids=batch["rejected_ids"],
            rejected_mask=batch["rejected_mask"],
            beta=beta,
        )

        # Backward pass
        optimizer.zero_grad()
        loss.backward()

        # Gradient clipping for stability
        torch.nn.utils.clip_grad_norm_(policy_model.parameters(), max_norm=1.0)

        # Update weights
        optimizer.step()
        scheduler.step()

        # Store metrics
        metrics["epoch"] = epoch + 1
        metrics["lr"] = scheduler.get_last_lr()[0]
        all_metrics.append(metrics)

        # Logging
        print(f"\nEpoch {epoch + 1}/{num_epochs}")
        print(f"  Loss: {metrics['loss']:.4f}")
        print(f"  Accuracy: {metrics['accuracy']:.2%}")
        print(f"  Reward Margin: {metrics['reward_margin']:.4f}")
        print(f"  Chosen KL: {metrics['chosen_kl']:.4f}")
        print(f"  Rejected KL: {metrics['rejected_kl']:.4f}")

    # Generate sample AFTER training
    print("\n--- AFTER TRAINING ---")
    print(f"Prompt: {test_prompt}")
    print(f"Response: {generate_sample(policy_model, tokenizer, test_prompt)}")

    print("\n" + "=" * 60)
    print("TRAINING COMPLETE")
    print("=" * 60)

    return policy_model, all_metrics


# Run the training!
if __name__ == "__main__":
    trained_model, metrics = train_dpo(
        policy_model=policy_model,
        ref_model=ref_model,
        preferences=preferences,
        tokenizer=tokenizer,
        num_epochs=10,
        learning_rate=1e-5,
        beta=0.1,
    )

    # Final evaluation: compare generations
    print("\n" + "=" * 60)
    print("FINAL EVALUATION")
    print("=" * 60)

    test_prompts = [
        "Explain recursion briefly:",
        "What is machine learning?",
        "Write a short poem about AI:",
    ]

    for prompt in test_prompts:
        print(f"\nPrompt: {prompt}")
        print(f"Response: {generate_sample(trained_model, tokenizer, prompt)}")</pre></code>
                    </div>

                    <h5>Success Criteria:</h5>
                    <ul>
                        <li>DPO loss decreases during training</li>
                        <li>Chosen response log prob increases relative to rejected</li>
                        <li>Generated text shows improvement in quality/style</li>
                    </ul>
                </div>
            </div>

            <!-- Checkpoint Summary -->
            <h2 class="mt-4">Checkpoint Summary</h2>

            <div class="card" style="background: linear-gradient(135deg, rgba(16, 185, 129, 0.1) 0%, rgba(56, 239, 125, 0.1) 100%); border: 2px solid rgba(16, 185, 129, 0.3);">
                <h3>Key Takeaways</h3>

                <h4>Scale of Modern Training</h4>
                <ul>
                    <li>GPT-4 scale: ~25,000 GPUs, ~$100M, months of training</li>
                    <li>Compute grows 10x/year; Chinchilla law: ~20 tokens per parameter</li>
                </ul>

                <h4>Distributed Training</h4>
                <ul>
                    <li><strong>Data Parallel:</strong> Same model, different data - DDP/FSDP/ZeRO</li>
                    <li><strong>Pipeline Parallel:</strong> Split by layers, use across nodes</li>
                    <li><strong>Tensor Parallel:</strong> Split operations, use within node</li>
                    <li><strong>3D Parallelism:</strong> Combine all three for massive scale</li>
                </ul>

                <h4>Efficiency Techniques</h4>
                <ul>
                    <li><strong>Mixed Precision:</strong> BF16 preferred (same range as FP32, no scaling needed)</li>
                    <li><strong>Gradient Checkpointing:</strong> Trade compute for memory (~30-50% overhead)</li>
                </ul>

                <h4>Alignment</h4>
                <ul>
                    <li><strong>RLHF:</strong> SFT -> Reward Model -> PPO optimization</li>
                    <li><strong>DPO:</strong> Simpler alternative, directly optimizes on preferences</li>
                    <li><strong>Constitutional AI:</strong> Self-critique based on explicit principles</li>
                </ul>

                <h4>Infrastructure</h4>
                <ul>
                    <li>NVLink within node, InfiniBand across nodes</li>
                    <li>Fault tolerance and checkpointing are critical at scale</li>
                    <li>Electricity and cooling are major costs</li>
                </ul>
            </div>

            <!-- How This Connects Forward -->
            <h2 class="mt-4">How This Connects Forward</h2>

            <div class="card">
                <p>With training fundamentals covered, you are ready for:</p>

                <ul>
                    <li><strong>Module 7 (Optimization Hacks):</strong> Inference-time optimizations - KV caching, quantization, LoRA</li>
                    <li><strong>Module 8 (RAG):</strong> How to extend LLMs with external knowledge</li>
                    <li><strong>Module 12 (AI Engineering):</strong> Production deployment, monitoring, and iteration</li>
                </ul>

                <p>Understanding training at scale helps you make better decisions about when to fine-tune vs use RAG, how to choose model sizes, and what infrastructure you need for your AI applications.</p>
            </div>

            <div class="flex flex-between mt-4">
                <a href="module-05.html" class="btn btn-secondary">&larr; Module 5: LLM Coding</a>
                <a href="module-07.html" class="btn btn-primary">Module 7: Optimization Hacks &rarr;</a>
            </div>
        </main>
    </div>

    <script src="../assets/js/app.js"></script>
    <script>
        // Initialize Mermaid
        mermaid.initialize({ startOnLoad: true, theme: 'default' });

        // Sidebar toggle
        document.addEventListener('DOMContentLoaded', function() {
            const sidebar = document.getElementById('sidebar');
            const sidebarToggle = document.getElementById('sidebarToggle');
            const sidebarOverlay = document.getElementById('sidebarOverlay');

            if (sidebarToggle) {
                sidebarToggle.addEventListener('click', () => {
                    sidebar.classList.toggle('open');
                    sidebarOverlay.classList.toggle('open');
                });
            }

            if (sidebarOverlay) {
                sidebarOverlay.addEventListener('click', () => {
                    sidebar.classList.remove('open');
                    sidebarOverlay.classList.remove('open');
                });
            }
        });
    </script>
</body>
</html>
